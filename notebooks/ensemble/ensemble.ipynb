{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble modeller\n",
    "\n",
    "# import various evidence layers\n",
    "# build frame of discernment\n",
    "# e.g. rescale images > 0.8 for each image\n",
    "# which images are gdv areas, which layers are non-gdv areas?\n",
    "\n",
    "# superset = [L, P, S, F, C]\n",
    "\n",
    "# L = GDV Likelihood\n",
    "# P = Phenometric (e.g. LIOT)\n",
    "# S = SDM\n",
    "# F = Fractional Map\n",
    "# C = CHM\n",
    "\n",
    "# determine if each layer is site, non-site.\n",
    "# assign bpa's \n",
    "# e.g. bird like sw aspect, so sw aspect gets 0.75, flat givem 0.5, rest (superset) gets 0.25\n",
    "# e.g. increasing signmoidal, 0 to 1\n",
    "# combine site hypotheses (e.g. site layers)\n",
    "# e.g. site (vege layer) = 0.8, rest (ignoreance) get 0.2. \n",
    "# e.g. site (aspect) =0.9, rest (ignoreance) get 0.1\n",
    "# prepare matrix for site hypothesies (vege, aspect)\n",
    "# cross multiply sites (veg x aspect) and ignorance\n",
    "# next matrix for non-sites 1, 2, cross multi,sum values\n",
    "# new matrix for non-sites 2, 3, cross multi,sum values\n",
    "# add results of those two into matrix, cross multi, then sum values\n",
    "# take result from site, non-site, superset, and put in new matrix\n",
    "# cross multi, sum up, divide by intersection \n",
    "# calc belief by summing any site values, calc disbelief by summing any non-site\n",
    "# do this per pixel\n",
    "# you get disbelief, plaus, belief interval (plaus - belief) and belief\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.7.2-CAPI-1.11.0 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datacube\n",
    "sys.path.append('../../../Scripts')\n",
    "from dea_datahandling import load_ard\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_plotting import display_map, rgb\n",
    "\n",
    "sys.path.append('../../modules')\n",
    "import gdvspectra, phenolopy, nicher, vegfrax, canopy, ensemble\n",
    "\n",
    "sys.path.append('../../shared')\n",
    "import satfetcher, tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster and ODC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46545</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/lewis/proxy/8787/status' target='_blank'>/user/lewis/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>13.11 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46545' processes=1 threads=2, memory=13.11 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/drivers/postgres/_connections.py:87: SADeprecationWarning: Calling URL() directly is deprecated and will be disabled in a future release.  The public constructor for URL is now the URL.create() method.\n",
      "  username=username, password=password,\n"
     ]
    }
   ],
   "source": [
    "# initialise the cluster. paste url into dask panel for more info.\n",
    "create_local_dask_cluster()\n",
    "\n",
    "# open up a datacube connection\n",
    "dc = datacube.Datacube(app='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study area and data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set study area, time range, show map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f868001a5c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing study area extent - yandi and roy hill\n",
    "#lat_extent, lon_extent = (-22.82901, -22.67901), (118.94980, 119.29979)  # yandi\n",
    "lat_extent, lon_extent = (-22.63461, -22.33461), (119.88111, 120.18111) # royhill\n",
    "\n",
    "# display onto interacrive map\n",
    "display_map(x=lon_extent, y=lat_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate GDV Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare DEA ODC satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide study area name\n",
    "study_area = 'royhill'\n",
    "\n",
    "# select start and end year range\n",
    "time_range = ('2016', '2020')\n",
    "\n",
    "# set datacube query parameters\n",
    "platform = 'landsat'\n",
    "bands = ['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2']\n",
    "#bands = ['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir_1', 'nbart_swir_2'] # sentinel\n",
    "min_gooddata = 0.90\n",
    "\n",
    "# fetch satellite data from dea ard product\n",
    "ds = satfetcher.load_dea_ard(platform=platform, \n",
    "                             bands=bands, \n",
    "                             x_extent=lon_extent, \n",
    "                             y_extent=lat_extent, \n",
    "                             time_range=time_range, \n",
    "                             min_gooddata=min_gooddata, \n",
    "                             use_dask=True)\n",
    "\n",
    "# rename dea bands to common standard\n",
    "ds = satfetcher.conform_dea_ard_band_names(ds=ds, platform=platform)\n",
    "\n",
    "# take a copy of dataset for cva later\n",
    "ds_backup = ds.copy(deep=True)\n",
    "\n",
    "# display dataset\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate standardised vege/moist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wet and dry season month(s). we will use several per season\n",
    "wet_month, dry_month = [1, 2, 3], [9, 10, 11]\n",
    "\n",
    "# get subset fo data for wet and dry season months\n",
    "ds = gdvspectra.subset_months(ds=ds, \n",
    "                              month=wet_month + dry_month,\n",
    "                              inplace=True)\n",
    "\n",
    "# calculate veg (mavi) and moist (ndmi) indices\n",
    "ds = tools.calculate_indices(ds=ds, \n",
    "                             index=['mavi', 'ndmi'], \n",
    "                             custom_name=['veg_idx', 'mst_idx'], \n",
    "                             rescale=True, \n",
    "                             drop=True)\n",
    "\n",
    "# perform resampling\n",
    "ds = gdvspectra.resample_to_wet_dry_medians(ds=ds, \n",
    "                                            wet_month=wet_month, \n",
    "                                            dry_month=dry_month,\n",
    "                                            inplace=True)\n",
    "\n",
    "# we have some calcs to make, persist now\n",
    "ds = ds.persist()\n",
    "\n",
    "# drop any years from dataset where wet and dry seasons missing\n",
    "ds = gdvspectra.drop_incomplete_wet_dry_years(ds)\n",
    "\n",
    "# todo - remove this compute when bug fixed in ver > 0.18.2\n",
    "ds = ds.compute()\n",
    "\n",
    "# fill any empty first, last years using back/forward fill\n",
    "ds = gdvspectra.fill_empty_wet_dry_edges(ds=ds,\n",
    "                                         wet_month=wet_month, \n",
    "                                         dry_month=dry_month,\n",
    "                                         inplace=True)\n",
    "\n",
    "# interpolate all missing pixels using full linear interpolation\n",
    "ds =  gdvspectra.interp_empty_wet_dry(ds=ds,\n",
    "                                      wet_month=wet_month, \n",
    "                                      dry_month=dry_month,\n",
    "                                      method='full', \n",
    "                                      inplace=True)\n",
    "\n",
    "# standardise data to invariant targets derived from dry times\n",
    "ds = gdvspectra.standardise_to_dry_targets(ds=ds, \n",
    "                                           dry_month=dry_month, \n",
    "                                           q_upper=0.99, \n",
    "                                           q_lower=0.05,\n",
    "                                           inplace=True)\n",
    "\n",
    "# calculate standardised seaonal similarity (diff between wet, dry per year)\n",
    "ds_similarity = gdvspectra.calc_seasonal_similarity(ds=ds,\n",
    "                                                    wet_month=wet_month,\n",
    "                                                    dry_month=dry_month,\n",
    "                                                    q_mask=0.9,\n",
    "                                                    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate likelihood model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate gdv likelihood model using wet, dry, similarity variables\n",
    "ds_like = gdvspectra.calc_likelihood(ds=ds, \n",
    "                                     ds_similarity=ds_similarity,\n",
    "                                     wet_month=wet_month, \n",
    "                                     dry_month=dry_month)\n",
    "\n",
    "# preview an all-time median of gdv likelihood. red is high likelihood\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ds_like['like'].median('time').plot(robust=False, cmap='jet')\n",
    "\n",
    "# create out file\n",
    "out_like_nc = '../../{0}_{1}_{2}_{3}_like.nc'.format(study_area,\n",
    "                                                     time_range[0],\n",
    "                                                     time_range[1],\n",
    "                                                     platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate field occurrence points for thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location of point shapefile with presence/absence column\n",
    "#shp_path = r'../GDVSDM/data_testing/presence_points/presence_points.shp'\n",
    "shp_path = r'../../data/gdvspectra/royhill_2_final_albers.shp'\n",
    "\n",
    "# read shapefile as pandas dataframe\n",
    "df_records = tools.read_shapefile(shp_path=shp_path)\n",
    "\n",
    "# subset to just x, y, pres/abse column\n",
    "df_records = tools.subset_records(df_records=df_records, p_a_column='GDV_ACT')\n",
    "\n",
    "# display dataframe\n",
    "#df_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform thresholding using standard deviation on all-time median likelihood\n",
    "ds_thresh = gdvspectra.threshold_likelihood(ds=ds_like.median('time', keep_attrs=True),\n",
    "                                            df=df_records, \n",
    "                                            num_stdevs=2.5, \n",
    "                                            res_factor=3, \n",
    "                                            if_nodata='any')\n",
    "\n",
    "# preview an all-time median of gdv likelihood thresholded\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ds_thresh.where(~ds_thresh.isnull(), 0.001)['like'].plot(robust=False, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export GDV likelihood and threshold for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export median likelihood to nc\n",
    "#tools.export_xr_as_nc(ds=ds_like['like'].median('time'), filename='ds_like.nc')\n",
    "\n",
    "# export threshold as nc\n",
    "#tools.export_xr_as_nc(ds_thresh, filename='ds_threshold.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate trends using Mann-Kendall trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask where gdv highly likely\n",
    "ds_mask = xr.where(~ds_thresh.isnull(), True, False)\n",
    "\n",
    "# do mk to find sig. inc/dec trends in high likelihood areas\n",
    "ds_mk = gdvspectra.perform_mk_original(ds=ds_like.where(ds_mask), \n",
    "                                       pvalue=None, \n",
    "                                       direction='both')\n",
    "\n",
    "# show mk trends. blue is increasing, red is decreasing\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ds_mk['tau'].plot(robust=True, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate slope using Theil-Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask where gdv highly likely\n",
    "ds_mask = xr.where(~ds_thresh.isnull(), True, False)\n",
    "\n",
    "# do theil sen slopes in high likelihood areas\n",
    "ds_ts = gdvspectra.perform_theilsen_slope(ds=ds_like.where(ds_mask), \n",
    "                                          alpha=0.95)\n",
    "\n",
    "# show mk trends. blue is increasing, red is decreasing\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ds_ts['theilsen'].plot(robust=True, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Phenometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and pre-process vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes our dask ds and calculates veg index from spectral bands\n",
    "ds = tools.calculate_indices(ds=ds_backup, \n",
    "                             index='mavi', \n",
    "                             custom_name='veg_idx', \n",
    "                             rescale=False, \n",
    "                             drop=True)\n",
    "\n",
    "# conform edges\n",
    "ds = phenolopy.conform_edge_dates(ds=ds)\n",
    "\n",
    "# resample to weekly medians\n",
    "ds = phenolopy.resample(ds=ds, \n",
    "                        interval='1W',\n",
    "                        inplace=True)\n",
    "\n",
    "# interpolate missing values\n",
    "ds = phenolopy.interpolate(ds=ds, \n",
    "                           method='full', \n",
    "                           inplace=True)\n",
    "\n",
    "# group into single year of weekly all-time medians\n",
    "ds = phenolopy.group(ds=ds, \n",
    "                     interval='week',\n",
    "                     inplace=True)\n",
    "\n",
    "# takes our dask ds and remove outliers from data using median method\n",
    "ds = phenolopy.remove_outliers(ds=ds, \n",
    "                               method='median', \n",
    "                               user_factor=2, \n",
    "                               z_pval=0.05)\n",
    "\n",
    "# take dataset and resample data to weekly medians (1WS)\n",
    "ds = phenolopy.resample(ds=ds, \n",
    "                        interval='1W',\n",
    "                        inplace=True)\n",
    "\n",
    "# remove any years outside of dominant year\n",
    "ds = phenolopy.remove_overshoot_times(ds=ds, max_times=3)\n",
    "\n",
    "# use savitsky-golay filter to smooth across time dimension\n",
    "ds = phenolopy.smooth(ds=ds, \n",
    "                      method='savitsky', \n",
    "                      window_length=3, \n",
    "                      polyorder=1, \n",
    "                      sigma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate phenometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute into memory\n",
    "ds = ds.compute()\n",
    "\n",
    "# set desired metrics\n",
    "#metrics = ['sos', 'eos', 'lios', 'sios', 'liot', 'siot']\n",
    "\n",
    "# calc phenometrics via phenolopy!\n",
    "ds = phenolopy.calc_phenometrics(ds=ds,\n",
    "                                 peak_metric='pos', \n",
    "                                 base_metric='vos', \n",
    "                                 method='seasonal_amplitude', \n",
    "                                 factor=0.2, \n",
    "                                 thresh_sides='one_sided', \n",
    "                                 abs_value=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display phenometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the metric to display\n",
    "metric_name = 'liot_values'\n",
    "fig = plt.figure(figsize=(9, 7), dpi=85)\n",
    "ds[metric_name].plot(robust=True, cmap='terrain_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate field occurrence points for thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location of point shapefile with presence/absence column\n",
    "#shp_path = r'../GDVSDM/data_testing/presence_points/presence_points.shp'\n",
    "shp_path = r'../../data/gdvspectra/royhill_2_final_albers.shp'\n",
    "\n",
    "# read shapefile as pandas dataframe\n",
    "df_records = tools.read_shapefile(shp_path=shp_path)\n",
    "\n",
    "# subset to just x, y, pres/abse column\n",
    "df_records = tools.subset_records(df_records=df_records, p_a_column='GDV_ACT')\n",
    "\n",
    "# display dataframe\n",
    "#df_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check AUC of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare metric for auc\n",
    "metric = 'vos_values'\n",
    "da = ds[metric].rename({'variable': 'time'}).to_dataset(promote_attrs=True)\n",
    "da = da.rename({metric: 'like'})\n",
    "\n",
    "# threshold to get auc\n",
    "gdvspectra.threshold_likelihood(ds=da,\n",
    "                                df=df_records, \n",
    "                                num_stdevs=2.5, \n",
    "                                res_factor=3, \n",
    "                                if_nodata='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to nc\n",
    "#tools.export_xr_as_nc(ds=ds, filename='ds_phenometrics.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Species Distribution Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get continuous rasters. wwe will use lidar\n",
    "folder_path = r'../../data/nicher/roy_lidar'\n",
    "rast_cont_list = nicher.get_files_from_path(folder_path)\n",
    "\n",
    "# drop any undesirable vars \n",
    "rast_cont_list.remove(folder_path + '/' + 'chm_lidar_10m.tif')\n",
    "rast_cont_list.remove(folder_path + '/' + 'dem_lidar_10m_fill.tif')\n",
    "rast_cont_list.remove(folder_path + '/' + 'dem_lidar_10m.tif')\n",
    "rast_cont_list.remove(folder_path + '/' + 'dem_lidar_10m_tri.tif')\n",
    "\n",
    "# load rasters as individual dataset variables\n",
    "ds_sdm = satfetcher.load_local_rasters(rast_path_list=rast_cont_list, \n",
    "                                       use_dask=True, \n",
    "                                       conform_nodata_to=-999)\n",
    "\n",
    "# compute dask - we need to make calculations\n",
    "ds_sdm = ds_sdm.compute()\n",
    "\n",
    "# set path to shapefile\n",
    "shp_path = r'../../data/nicher/presence_points/presence_points.shp'\n",
    "\n",
    "# extract point x and y from shapefile as pandas dataframe\n",
    "df_records = tools.read_shapefile(shp_path=shp_path)\n",
    "\n",
    "# subset columns\n",
    "df_presence = tools.subset_records(df_records=df_records, \n",
    "                                   p_a_column=None)\n",
    "\n",
    "# drop presence column\n",
    "df_presence = df_presence.drop('actual', axis='columns')\n",
    "\n",
    "# generate absences using dataset pixels and occurrence coords\n",
    "df_absence = nicher.generate_absences(ds=ds_sdm, \n",
    "                                      occur_shp_path=shp_path,\n",
    "                                      buff_m=250, \n",
    "                                      res_factor=3)\n",
    "\n",
    "# extract values for presence points\n",
    "df_presence_data = tools.extract_xr_values(ds=ds_sdm, \n",
    "                                           coords=df_presence, \n",
    "                                           keep_xy=False, \n",
    "                                           res_factor=3)\n",
    "\n",
    "# do same for absence points\n",
    "df_absence_data = tools.extract_xr_values(ds=ds_sdm, \n",
    "                                          coords=df_absence, \n",
    "                                          keep_xy=False, \n",
    "                                          res_factor=3)\n",
    "\n",
    "# remove all presence records containing nodata values\n",
    "df_presence_data = tools.remove_nodata_records(df_records=df_presence_data,\n",
    "                                               nodata_value=ds_sdm.nodatavals)\n",
    "\n",
    "# remove all absence records containing nodata values\n",
    "df_absence_data = tools.remove_nodata_records(df_records=df_absence_data,\n",
    "                                               nodata_value=ds_sdm.nodatavals)\n",
    "\n",
    "# take pres and abse records and combine, add new pres/abse column\n",
    "df_pres_abse_data = nicher.combine_pres_abse_records(df_presence=df_presence_data, \n",
    "                                                     df_absence=df_absence_data)\n",
    "\n",
    "# generate the matrix. < 0.6 weak collinearity, 0.6-0.8 moderate, >= 0.8 strong\n",
    "nicher.generate_correlation_matrix(df_records=df_pres_abse_data,\n",
    "                                   show_fig=True,\n",
    "                                   show_text=False)\n",
    "\n",
    "# generate vif scores. 1 = No multicolinearity, 1-5 = moderate, > 5 = high, > 10 = Remove\n",
    "nicher.generate_vif_scores(df_records=df_pres_abse_data)\n",
    "plt.show()\n",
    "\n",
    "# create a random forest estimator using default sklearn parameters\n",
    "estimator = nicher.create_estimator(estimator_type='rf', \n",
    "                                    n_estimators=100)\n",
    "\n",
    "# generate SDM with 5 replicates and 10% training-testing split\n",
    "ds_sdm = nicher.generate_sdm(ds=ds_sdm, \n",
    "                             df_records=df_pres_abse_data, \n",
    "                             estimator=estimator, \n",
    "                             rast_cont_list=rast_cont_list, \n",
    "                             rast_cate_list=None, \n",
    "                             replicates=5, \n",
    "                             test_ratio=0.1, \n",
    "                             equalise_test_set=False, \n",
    "                             calc_accuracy_stats=True)\n",
    "\n",
    "# show results\n",
    "fig = plt.figure(figsize=(9, 7), dpi=85)\n",
    "ds_sdm['sdm_mean'].plot(robust=False, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to nc\n",
    "#tools.export_xr_as_nc(ds=ds_sdm, filename='ds_sdm.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Veg Frax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Landsat satellite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate veg (mavi) and moist (ndmi) indices\n",
    "ds_raw = tools.calculate_indices(ds=ds_backup, \n",
    "                                 index=['tcg', 'tcb', 'tcw'], \n",
    "                                 custom_name=None, \n",
    "                                 rescale=False, \n",
    "                                 drop=True)\n",
    "\n",
    "# load into memory now - we have values to modify!\n",
    "ds_raw = ds_raw.median('time', keep_attrs=True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare classified hi-def raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to high-resolution classified image (e.g. 10m Sentinel 2 or 1m WV)\n",
    "rast_class = r'../../data/vegfrax/class/Vegetation_Mapping_Mine_20181121_rasterised_albers.tif'\n",
    "ds_class = satfetcher.load_local_rasters(rast_path_list=[rast_class], \n",
    "                                         use_dask=True, \n",
    "                                         conform_nodata_to=-128)\n",
    "\n",
    "# do basic preparations (dtype, rename, checks)\n",
    "ds_class = vegfrax.prepare_classified_xr(ds=ds_class)\n",
    "\n",
    "# subset high to low extent\n",
    "ds_class = tools.clip_xr_to_xr(ds_a=ds_class, \n",
    "                               ds_b=ds_raw)\n",
    "\n",
    "# set them manually (make sure you include 0 if reclassifying)\n",
    "req_class = [1, 3, 13, 14, 0]\n",
    "ds_class = vegfrax.reclassify_xr(ds=ds_class, \n",
    "                                 req_class=req_class,\n",
    "                                 merge_classes=True,\n",
    "                                 inplace=True)\n",
    "\n",
    "# get list of all classes...\n",
    "req_class = vegfrax.get_xr_classes(ds_class)\n",
    "\n",
    "# load into memory now - we have values to modify!\n",
    "ds_class = ds_class.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random samples within area overlap between raw and classified rasters\n",
    "num_samples = 500\n",
    "df_samples = vegfrax.generate_strat_random_samples(ds_raw=ds_raw,\n",
    "                                                   ds_class=ds_class, \n",
    "                                                   req_class=req_class,\n",
    "                                                   num_samples=num_samples)\n",
    "\n",
    "# extract pixel values from raw, low resolution rasters at each point\n",
    "df_extract = tools.extract_xr_values(ds=ds_raw, \n",
    "                                     coords=df_samples, \n",
    "                                     keep_xy=True)\n",
    "\n",
    "# remove any points containing a nodata value\n",
    "df_extract_clean = tools.remove_nodata_records(df_extract, \n",
    "                                               nodata_value=ds_raw.nodatavals)\n",
    "\n",
    "# generate focal windows and extract pixels from class raster\n",
    "df_windows = vegfrax.create_frequency_windows(ds_raw=ds_raw, \n",
    "                                              ds_class=ds_class, \n",
    "                                              df_records=df_extract_clean)\n",
    "\n",
    "# transform raw focal window pixel classes and counts to unique classes and frequencies at each point\n",
    "df_freqs = vegfrax.convert_window_counts_to_freqs(df_windows=df_windows, \n",
    "                                                  nodata_value=ds_class.nodatavals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform FCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set desired output classes. keep empty to produce all classes. could put 1, 2 for classes 1 and 2.\n",
    "override_classes = ['1']\n",
    "\n",
    "# prepare data for analysis - prepare classes, nulls, normalise frequencies\n",
    "df_data = vegfrax.prepare_freqs_for_analysis(ds_raw=ds_raw, \n",
    "                                             ds_class=ds_class, \n",
    "                                             df_freqs=df_freqs, \n",
    "                                             override_classes=override_classes)\n",
    "\n",
    "# perform fca\n",
    "ds_preds = vegfrax.perform_fca(ds_raw=ds_raw, \n",
    "                               ds_class=ds_class, \n",
    "                               df_data=df_data, \n",
    "                               df_extract_clean=df_extract_clean, \n",
    "                               n_estimators=100,\n",
    "                               n_validations=10)\n",
    "\n",
    "# create fig\n",
    "class_label = '1'\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ds_preds[class_label].plot(robust=False, cmap='terrain_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate field occurrence points for thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set location of point shapefile with presence/absence column\n",
    "#shp_path = r'../GDVSDM/data_testing/presence_points/presence_points.shp'\n",
    "shp_path = r'../../data/gdvspectra/royhill_2_final_albers.shp'\n",
    "\n",
    "# read shapefile as pandas dataframe\n",
    "df_records = tools.read_shapefile(shp_path=shp_path)\n",
    "\n",
    "# subset to just x, y, pres/abse column\n",
    "df_records = tools.subset_records(df_records=df_records, p_a_column='GDV_ACT')\n",
    "\n",
    "# display dataframe\n",
    "#df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare metric for auc\n",
    "ds_preds.attrs = ds.attrs\n",
    "da = ds_preds.rename({class_label: 'like'})\n",
    "\n",
    "# threshold to get auc\n",
    "gdvspectra.threshold_likelihood(ds=da,\n",
    "                                df=df_records, \n",
    "                                num_stdevs=2.5, \n",
    "                                res_factor=3, \n",
    "                                if_nodata='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths of rasters from arcgis\n",
    "like_path = r'../../data/ensemble/royhill/like.tif'\n",
    "pheno_path = r'../../data/ensemble/royhill/pheno_liot.tif'\n",
    "sdm_path = r'../../data/ensemble/royhill/sdm_lidar.tif'\n",
    "frax_path = r'../../data/ensemble/royhill/vegfrax_class_3_vs_0.tif'\n",
    "chm_path = r'../../data/ensemble/royhill/chm_lidar_10m.tif'\n",
    "\n",
    "# create empty dict \n",
    "in_dict = {\n",
    "    'like_path': like_path,\n",
    "    'pheno_path': pheno_path,\n",
    "    'sdm_path': sdm_path,\n",
    "    'frax_path': frax_path,\n",
    "    'chm_path': chm_path,\n",
    "}\n",
    "\n",
    "# remove any empty keys\n",
    "for k in list(in_dict.keys()):\n",
    "    if in_dict[k] in ['', None]:\n",
    "        del in_dict[k]\n",
    "        \n",
    "# check if anything remains\n",
    "if len(in_dict) == 0:\n",
    "    raise ValueError('No valid paths provided.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting rasters to an xarray dataset.\n",
      "Converted raster to xarray data array: like\n",
      "Rasters converted to dataset successfully.\n",
      "\n",
      "Converting rasters to an xarray dataset.\n",
      "Converted raster to xarray data array: pheno_liot\n",
      "Rasters converted to dataset successfully.\n",
      "\n",
      "Converting rasters to an xarray dataset.\n",
      "Converted raster to xarray data array: sdm_lidar\n",
      "Rasters converted to dataset successfully.\n",
      "\n",
      "Converting rasters to an xarray dataset.\n",
      "Converted raster to xarray data array: vegfrax_class_3_vs_0\n",
      "Rasters converted to dataset successfully.\n",
      "\n",
      "Converting rasters to an xarray dataset.\n",
      "Converted raster to xarray data array: chm_lidar_10m\n",
      "Rasters converted to dataset successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iter dict and replace path with lazy loaded ds\n",
    "ds_dict = {}\n",
    "for k, v in in_dict.items():\n",
    "    var_name = k.replace('_path', '')\n",
    "    ds = satfetcher.load_local_rasters(rast_path_list=v, \n",
    "                                       use_dask=True, \n",
    "                                       conform_nodata_to=np.nan)\n",
    "    # convert to array\n",
    "    ds_dict[var_name] = ds.to_array().squeeze(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pheno as our target layer for resampling , else like\n",
    "if 'pheno' in ds_dict:\n",
    "    resampler_var = 'pheno'\n",
    "elif 'like' in ds_dict:\n",
    "    resampler_var = 'like'\n",
    "else:\n",
    "    raise ValueError('Ensemble must have a likelihood or phenometric var(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate each layer and resample to target layer\n",
    "for k in list(ds_dict.keys()):\n",
    "    if k != resampler_var:\n",
    "        ds_dict[k] = tools.resample_xr(ds_from=ds_dict[k], \n",
    "                                       ds_to=ds_dict[resampler_var], \n",
    "                                       resampling='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter each and perform sigmoidal\n",
    "for k in list(ds_dict.keys()):\n",
    "    if k == 'like':\n",
    "        ds_dict[k] = canopy.inc_sigmoid(ds=ds_dict[k], \n",
    "                                        a=0.3, \n",
    "                                        b=float(ds_dict[k].max()))\n",
    "    \n",
    "    elif k == 'pheno':\n",
    "        ds_dict[k] = canopy.inc_sigmoid(ds=ds_dict[k], \n",
    "                                        a=5, \n",
    "                                        b=float(ds_dict[k].max()))\n",
    "        \n",
    "    elif k == 'sdm':\n",
    "        ds_dict[k] = canopy.inc_sigmoid(ds=ds_dict[k], \n",
    "                                        a=0.1, \n",
    "                                        b=float(ds_dict[k].max()))\n",
    "        \n",
    "    elif k == 'chm':\n",
    "        ds_dict[k] = canopy.bell_sigmoid(ds=ds_dict[k], \n",
    "                                         a=1, \n",
    "                                         bc=6,\n",
    "                                         d=11)\n",
    "        \n",
    "    elif k == 'frax':\n",
    "        ds_dict[k] = canopy.dec_sigmoid(ds=ds_dict[k],\n",
    "                                        c=float(ds_dict[k].min()),\n",
    "                                        d=float(ds_dict[k].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pheno': <xarray.DataArray 'stack-234e19b4f96e460d73156c9da24c7378' (y: 1208, x: 1121)>\n",
       " dask.array<where, shape=(1208, 1121), dtype=float32, chunksize=(1208, 1121), chunktype=numpy.ndarray>\n",
       " Coordinates:\n",
       "   * y        (y) float64 -2.46e+06 -2.46e+06 -2.46e+06 ... -2.496e+06 -2.496e+06\n",
       "   * x        (x) float64 -1.236e+06 -1.236e+06 ... -1.202e+06 -1.202e+06,\n",
       " 'sdm': <xarray.DataArray 'stack-bf5225d3e75bdd754da8cd996582e654' (y: 1208, x: 1121)>\n",
       " dask.array<where, shape=(1208, 1121), dtype=float32, chunksize=(1208, 1121), chunktype=numpy.ndarray>\n",
       " Coordinates:\n",
       "   * x        (x) float64 -1.236e+06 -1.236e+06 ... -1.202e+06 -1.202e+06\n",
       "   * y        (y) float64 -2.46e+06 -2.46e+06 -2.46e+06 ... -2.496e+06 -2.496e+06}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dempster_shafer(ds_dict):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # if all vars provided...\n",
    "    bpa_list = ['like', 'pheno', 'sdm', 'chm', 'frax']\n",
    "    if len(ds_dict) == len(bpa_list):\n",
    "        if all(e in ds_dict for e in bpa_list):\n",
    "            print('Performing Dempster-Shafer on all variables.')\n",
    "            return ensemble.bpa_all(ds_dict)\n",
    "            \n",
    "    # if like, sdm, chm, frax provided...\n",
    "    bpa_list = ['like', 'sdm', 'chm', 'frax']\n",
    "    if len(ds_dict) == len(bpa_list):\n",
    "        if all(e in ds_dict for e in bpa_list):\n",
    "            print('Performing Dempster-Shafer on like, sdm, chm and frax variables.')\n",
    "            return ensemble.bpa_lscv(ds_dict)\n",
    "\n",
    "    # if pheno, sdm, chm, frax provided...\n",
    "    bpa_list = ['pheno', 'sdm', 'chm', 'frax']\n",
    "    if len(ds_dict) == len(bpa_list):\n",
    "        if all(e in ds_dict for e in bpa_list):\n",
    "            print('Performing Dempster-Shafer on pheno, sdm, chm and frax variables.')\n",
    "            return ensemble.bpa_pscv(ds_dict)\n",
    "        \n",
    "    # if pheno, sdm, frax provided...\n",
    "    bpa_list = ['pheno', 'sdm', 'frax']\n",
    "    if len(ds_dict) == len(bpa_list):\n",
    "        if all(e in ds_dict for e in bpa_list):\n",
    "            print('Performing Dempster-Shafer on pheno, sdm and frax variables.')\n",
    "            return ensemble.bpa_psv(ds_dict)    \n",
    "    \n",
    "    # if pheno, chm, frax provided...\n",
    "    bpa_list = ['pheno', 'chm', 'frax']\n",
    "    if len(ds_dict) == len(bpa_list):\n",
    "        if all(e in ds_dict for e in bpa_list):\n",
    "            print('Performing Dempster-Shafer on pheno, chm and frax variables.')\n",
    "            return ensemble.bpa_pcv(ds_dict)\n",
    "        \n",
    "    # do pheno, sdm, chm\n",
    "        \n",
    "    # do pheno, sdm\n",
    "    \n",
    "    # do pheno, chm\n",
    "\n",
    "    # if pheno and frax provided...\n",
    "    bpa_list = ['pheno', 'frax']\n",
    "    if len(ds_dict) == len(bpa_list):\n",
    "        if all(e in ds_dict for e in bpa_list):\n",
    "            print('Performing Dempster-Shafer on pheno and frax variables.')\n",
    "            return ensemble.bpa_pv(ds_dict)    \n",
    "\n",
    "\n",
    "# remove chm < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dempster = perform_dempster_shafer(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dempster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load like var\n",
    "ds_like = satfetcher.load_local_rasters(rast_path_list=r'../../data/ensemble/royhill/like.tif', \n",
    "                                        use_dask=True, \n",
    "                                        conform_nodata_to=np.nan)\n",
    "\n",
    "# load liot var\n",
    "ds_liot = satfetcher.load_local_rasters(rast_path_list=r'../../data/ensemble/royhill/pheno_liot.tif', \n",
    "                                        use_dask=True, \n",
    "                                        conform_nodata_to=np.nan)\n",
    "\n",
    "# load sdm lidar\n",
    "ds_sdm = satfetcher.load_local_rasters(rast_path_list=r'../../data/ensemble/royhill/sdm_lidar.tif', \n",
    "                                       use_dask=True, \n",
    "                                       conform_nodata_to=np.nan)\n",
    "\n",
    "# load veg frax\n",
    "ds_frax = satfetcher.load_local_rasters(rast_path_list=r'../../data/ensemble/royhill/vegfrax_class_3_vs_0.tif', \n",
    "                                        use_dask=True, \n",
    "                                        conform_nodata_to=np.nan)\n",
    "\n",
    "# load chm\n",
    "ds_chm = satfetcher.load_local_rasters(rast_path_list=r'../../data/ensemble/royhill/chm_lidar_10m.tif', \n",
    "                                        use_dask=True, \n",
    "                                        conform_nodata_to=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample higher to lower\n",
    "#sdm to like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
