{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise COG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.7.2-CAPI-1.11.0 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import html\n",
    "import requests\n",
    "import gdal\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "sys.path.append('../../../Scripts')\n",
    "from dea_dask import create_local_dask_cluster\n",
    "\n",
    "sys.path.append('../../shared')\n",
    "import satfetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:35979</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/lewis/proxy/8787/status' target='_blank'>/user/lewis/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>13.11 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35979' processes=1 threads=2, memory=13.11 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialise the cluster\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get study area polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load study area geometry as geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read study area and get bounds as a list\n",
    "gdf = gpd.read_file('../../data/cog/yandisa.geojson')\n",
    "gdf_bounds = gdf.bounds.values[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set STAC Search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get satellite collection on dea. todo get from user in arcgis, sentinel 2\n",
    "collections = [\n",
    "    'ga_ls5t_ard_3', \n",
    "    'ga_ls7e_ard_3',\n",
    "    'ga_ls8c_ard_3'\n",
    "]\n",
    "\n",
    "# set required bands\n",
    "bands = [\n",
    "    'nbart_blue', \n",
    "    'nbart_green', \n",
    "    'nbart_red', \n",
    "    'nbart_nir',\n",
    "    'nbart_swir_1',\n",
    "    'nbart_swir_2',\n",
    "    'oa_fmask'\n",
    "]\n",
    "\n",
    "# get satellite collection date range, convert to stac. todo get from user in arcgis\n",
    "start_dt, end_dt = '1990-01-01', '1995-12-31'\n",
    "\n",
    "# bring it all together for a query\n",
    "query = {\n",
    "    'collections': collections,\n",
    "    'datetime': '{0}/{1}'.format(start_dt, end_dt),\n",
    "    'bbox': gdf_bounds,\n",
    "    'query': {'eo:cloud_cover': {'lt': 5}}, #this doesnt work\n",
    "    'limit': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch DEA Public Data via STAC Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146 satellite scenes in total.\n"
     ]
    }
   ],
   "source": [
    "# set stac endpoint\n",
    "search_endpoint = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'\n",
    "\n",
    "# send and get request for stac json using \n",
    "stac_response = requests.post(search_endpoint, json=query)\n",
    "\n",
    "# check for response empty errors, convert to json if so\n",
    "if stac_response.ok:\n",
    "    stac_response = stac_response.json()\n",
    "    num_items = len(stac_response.get('features'))\n",
    "    print('Found {0} satellite scenes in total.'.format(num_items))\n",
    "else:\n",
    "    raise ValueError('Could not connect to DEA STAC SEARCH endpoint.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate STAC response and remove cloud cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 23 satellite scenes due to clouds.\n",
      "Total of 123 satellite scenes remaining.\n"
     ]
    }
   ],
   "source": [
    "# set max cloud cover (0 - 100)\n",
    "max_cloud = 25\n",
    "\n",
    "# get num of all stac scenes\n",
    "num_all_items = len(stac_response.get('features'))\n",
    "\n",
    "feat_list = []\n",
    "for feat in stac_response.get('features'):\n",
    "    if max_cloud > float(feat.get('properties').get('eo:cloud_cover')):\n",
    "        feat_list.append(feat)\n",
    "        \n",
    "# count cloud less scenes and compare\n",
    "if feat_list:\n",
    "    num_clean_items = len(feat_list)\n",
    "    print('Removed {0} satellite scenes due to clouds.'.format(num_all_items - num_clean_items))\n",
    "    print('Total of {0} satellite scenes remaining.'.format(num_clean_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build VRTs for each band of each scene in STAC response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting landsat vrts for each relevant bands.\n",
      "Building landsat vrt list for band: nbart_blue.\n",
      "Building landsat vrt list for band: nbart_green.\n",
      "Building landsat vrt list for band: nbart_red.\n",
      "Building landsat vrt list for band: nbart_nir.\n",
      "Building landsat vrt list for band: nbart_swir_1.\n",
      "Building landsat vrt list for band: nbart_swir_2.\n",
      "Building landsat vrt list for band: oa_fmask.\n",
      "Got 7 landsat vrt band lists successfully.\n"
     ]
    }
   ],
   "source": [
    "# get dict of band names and associated vrt lists\n",
    "band_vrt_dict = satfetcher.get_dea_landsat_vrt_dict(feat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine VRTs into file for each band set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining VRTs into single VRTs per band.\n",
      "Combining VRTs into temp. file for band: nbart_blue.\n",
      "Combining VRTs into temp. file for band: nbart_green.\n",
      "Combining VRTs into temp. file for band: nbart_red.\n",
      "Combining VRTs into temp. file for band: nbart_nir.\n",
      "Combining VRTs into temp. file for band: nbart_swir_1.\n",
      "Combining VRTs into temp. file for band: nbart_swir_2.\n",
      "Combining VRTs into temp. file for band: oa_fmask.\n",
      "Combined 7 band vrt lists successfully.\n"
     ]
    }
   ],
   "source": [
    "# loop each band and combine vrts into one per band\n",
    "vrt_file_dict = satfetcher.combine_vrts_per_band(band_vrt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract datetimes from vrt file for each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datetimes for VRTs per band.\n",
      "Extracting datetimes from VRTs for band: nbart_blue.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_green.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_red.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_nir.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_swir_1.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_swir_2.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: oa_fmask.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracted 7 band vrt datetimes successfully.\n"
     ]
    }
   ],
   "source": [
    "# loop each band and extract datetimes for each\n",
    "vrt_dt_dict = satfetcher.get_vrt_file_datetimes(vrt_file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine VRT files and datetimes into a single dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining vrt files and datetimes per band.\n",
      "Combined vrt files and datetimes per band successfully.\n"
     ]
    }
   ],
   "source": [
    "# combine vrt files and datetimes for each band into final dict\n",
    "vrt_dict = satfetcher.prepare_full_vrt_dicts(vrt_file_dict, vrt_dt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build xarray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: do bounding box here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building an xarray dataset from vrt files and datetimes.\n",
      "Working on dataset for band: nbart_blue\n",
      "Working on dataset for band: nbart_green\n",
      "Working on dataset for band: nbart_red\n",
      "Working on dataset for band: nbart_nir\n",
      "Working on dataset for band: nbart_swir_1\n",
      "Working on dataset for band: nbart_swir_2\n",
      "Working on dataset for band: oa_fmask\n",
      "Built an xarray dataset successfully.\n"
     ]
    }
   ],
   "source": [
    "# create in-memory dataset from vrt dictionary\n",
    "ds = satfetcher.build_xr_datasets(vrt_dict=vrt_dict)\n",
    "\n",
    "# show ds\n",
    "#ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset size of dataset to study area bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset func\n",
    "ds = ds.isel(x=slice(1500, 2500), y=slice(1500, 2500))\n",
    "\n",
    "# show ds\n",
    "#ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ds['oa_fmask'].astype('Int8')\n",
    "ds = ds.to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test download times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try raw, without dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed testing without dask distributed\n",
    "%time ds = ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try raw, without dask but with threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed testing without dask distributed\n",
    "%time ds = ds.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try raw, without dask but with processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed testing without dask distributed\n",
    "%time ds = ds.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try dask, with distributed scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask\n",
    "#from dask.distributed import Client\n",
    "#client = Client(processes=True)\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 47 secs with processes=false, 21 secs when True\n",
    "%time ds = ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try dask data arrays split and futures used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures \n",
    "\n",
    "# create compute func\n",
    "def compute_da(da):\n",
    "    print('Computing band: {} at date: {}'.format(da.name, da['time'].values))\n",
    "    return da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split ds into seperate das\n",
    "da_list = []\n",
    "for dt in ds['time']:\n",
    "    da = ds.sel(time=dt)\n",
    "    da_list.append(da)\n",
    "    \n",
    "# try parallel load of all bands\n",
    "num_cores = 2\n",
    "with concurrent.futures.ThreadPoolExecutor(num_cores) as executor:\n",
    "    %time da_list = list(executor.map(compute_da, da_list))\n",
    "    \n",
    "ds = xr.concat(da_list, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 cores = Wall time: 14min 23s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try dask dataset split into seperate da based on band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc oa_fmask first on its own and determine % of cloud pixels\n",
    "# remove those > threshold from dask set\n",
    "# compute dask det\n",
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is th one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures \n",
    "\n",
    "# create compute func\n",
    "def compute_da_full(da):\n",
    "    print('Computing data array for band: {}.'.format(da.name))\n",
    "    return da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing data array for band: oa_fmask.\n",
      "CPU times: user 1.7 s, sys: 351 ms, total: 2.05 s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "# get list of each band seperated out\n",
    "da_list = [ds[band] for band in ds]\n",
    "\n",
    "# try parallel load of all bands\n",
    "num_cores = 2\n",
    "with concurrent.futures.ThreadPoolExecutor(num_cores) as executor:\n",
    "    %time da_list = list(executor.map(compute_da_full, da_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = da_list[0].to_dataset()\n",
    "mask = xr.where(ds['oa_fmask'].isin([1, 4, 5]), 1, 0)\n",
    "\n",
    "cloud_pct = (100 - (mask.sum(['x', 'y']) / mask.count(['x', 'y'])) * 100).astype('Int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-2766593e4792>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-2766593e4792>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    100 - ((mask.sum(['x', 'y']) / mask.count(['x', 'y']))) * 100)\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "100 - ((mask.sum(['x', 'y']) / mask.count(['x', 'y']))) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with parallel initialised at start\n",
    "# 2 cores Wall time: 7min 35s\n",
    "# 4 cores Wall time: 4min 30s\n",
    "# 7 cores Wall time: 4min 19s\n",
    "\n",
    "# without parallel initialised at start\n",
    "# 1 cores Wall time: 4min 22s\n",
    "# 2 cores Wall time: 4min 28s\n",
    "# 4 cores Wall time: 4min 41s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this to auto gen vrt to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good test env\n",
    "vrt_1_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/02/09/ga_ls5t_nbart_3-0-0_112076_1990-02-09_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/02/09/ga_ls5t_nbart_3-0-0_112076_1990-02-09_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/02/09/ga_ls5t_nbart_3-0-0_112076_1990-02-09_final_band03.tif']\n",
    "vrt1 = gdal.BuildVRT('vrt_1.vrt', vrt_1_urls, separate=True).FlushCache()\n",
    "\n",
    " \n",
    "vrt_2_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/13/ga_ls5t_nbart_3-0-0_112076_1990-03-13_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/13/ga_ls5t_nbart_3-0-0_112076_1990-03-13_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/13/ga_ls5t_nbart_3-0-0_112076_1990-03-13_final_band03.tif']\n",
    "vrt2 = gdal.BuildVRT('vrt_2.vrt', vrt_2_urls, separate=True).FlushCache()\n",
    "\n",
    "\n",
    "vrt_3_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/111/076/1990/03/22/ga_ls5t_nbart_3-0-0_111076_1990-03-22_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/111/076/1990/03/22/ga_ls5t_nbart_3-0-0_111076_1990-03-22_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/111/076/1990/03/22/ga_ls5t_nbart_3-0-0_111076_1990-03-22_final_band03.tif']\n",
    "vrt3 = gdal.BuildVRT('vrt_3.vrt', vrt_3_urls, separate=True).FlushCache()\n",
    "\n",
    "\n",
    "vrt_4_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/29/ga_ls5t_nbart_3-0-0_112076_1990-03-29_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/29/ga_ls5t_nbart_3-0-0_112076_1990-03-29_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/29/ga_ls5t_nbart_3-0-0_112076_1990-03-29_final_band03.tif']\n",
    "vrt4 = gdal.BuildVRT('vrt_4.vrt', vrt_4_urls, separate=True).FlushCache()\n",
    "\n",
    "# add to list\n",
    "vrt_list = ['vrt_1.vrt', 'vrt_2.vrt', 'vrt_3.vrt', 'vrt_4.vrt']\n",
    "vrt_out = gdal.BuildVRT('vrt_all.vrt', vrt_list, separate=False, bandList=[1]).FlushCache()\n",
    "\n",
    "\n",
    "# read it in to memory and decode it\n",
    "#vrt_all = tmp.read().decode(\"utf-8\")\n",
    "\n",
    "# setup chunks\n",
    "chunks = {'band': 1, 'x': 'auto', 'y': 'auto'}\n",
    "ds = xr.open_rasterio('vrt_all.vrt', chunks=chunks)\n",
    "ds = ds.isel(x=slice(2500, 3000), y=slice(2500, 3000))\n",
    "ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
