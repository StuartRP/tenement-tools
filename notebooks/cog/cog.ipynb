{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise COG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "delete this!\n",
      "remove this\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# imports todo - fix this up properly\n",
    "import os, certifi\n",
    "os.environ['GDAL_DATA']  = r'C:\\Program Files\\ArcGIS\\Pro\\Resources\\pedata\\gdaldata'\n",
    "os.environ.setdefault(\"CURL_CA_BUNDLE\", certifi.where())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#import gdal\n",
    "#import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import xarray as xr\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#import dask\n",
    "#import dask.array as da\n",
    "\n",
    "#from datetime import datetime\n",
    "#from lxml import etree\n",
    "\n",
    "sys.path.append('../../modules')\n",
    "import cog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Globals (i.e. AWS keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globals\n",
    "AWS_KEY = ''\n",
    "AWS_SECRET = ''\n",
    "STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set STAC query parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get satellite collection on dea. todo get from user in arcgis, sentinel 2\n",
    "collections = [\n",
    "    'ga_ls5t_ard_3', \n",
    "    'ga_ls7e_ard_3',\n",
    "    'ga_ls8c_ard_3'\n",
    "]\n",
    "\n",
    "# exclude slc\n",
    "slc_off = False\n",
    "\n",
    "# get satellite collection date range, convert to stac. todo get from user in arcgis\n",
    "start_dt, end_dt = '1990-01-01', '1995-12-31'\n",
    "\n",
    "# set temp boundary for testing\n",
    "bbox = [\n",
    "    118.92837524414061,\n",
    "    -22.816061209792938,\n",
    "    119.16526794433592,\n",
    "    -22.68118293381927\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch STAC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# fetch results\n",
    "feats = cog.fetch_stac_data(stac_endpoint=STAC_ENDPOINT, \n",
    "                       collections=collections, \n",
    "                       start_dt=start_dt, \n",
    "                       end_dt=end_dt, \n",
    "                       bbox=bbox,\n",
    "                       slc_off=slc_off,\n",
    "                       limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set COG parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set required bands\n",
    "assets = [\n",
    "    'nbart_blue', \n",
    "    'nbart_green', \n",
    "    'nbart_red', \n",
    "    'nbart_nir',\n",
    "    'nbart_swir_1',\n",
    "    'nbart_swir_2',\n",
    "    'oa_fmask'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare raw stac into useable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# convert raw stac into dict with coord reproject, etc.\n",
    "meta, asset_table = cog.prepare_data(feats, \n",
    "                                     assets=assets,\n",
    "                                     bounds_latlon=bbox, \n",
    "                                     bounds=None, \n",
    "                                     epsg=3577, \n",
    "                                     resolution=30, \n",
    "                                     snap_bounds=True,\n",
    "                                     force_dea_http=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO LOOK AT THIS FUNC AND MAKE IT YOURS\n",
    "da = cog.convert_to_dask(meta=meta, \n",
    "                         asset_table=asset_table, \n",
    "                         chunksize=512, \n",
    "                         resampling='nearest', \n",
    "                         dtype='int16', \n",
    "                         fill_value=-999, \n",
    "                         rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, convert stac items to plain dicts\n",
    "# plain_items = items_to_plain\n",
    "# just does this:\n",
    "#plain_items = [item._data for item in items]\n",
    "\n",
    "# sort whole dict by datetime via sorted func\n",
    "#plain_items = sorted(\n",
    "    #plain_items,\n",
    "    #key=lambda item: item[\"properties\"].get('datetime', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START PREPARE FUNC\n",
    "# now we call prepare itmes function\n",
    "#out_epsg = epsg\n",
    "#out_bounds = bounds # not bounds_latlon, this gets none if latlon bounds used\n",
    "\n",
    "#if resolution is not None and not isinstance(resolution, tuple):\n",
    "    #resolution = (resolution, resolution)\n",
    "#out_resolutions_xy = resolution\n",
    "\n",
    "# dont need to do all the work with assets, we always want same\n",
    "#asset_ids = assets\n",
    "\n",
    "# creates a table with structure for \n",
    "#ASSET_TABLE_DT = np.dtype([(\"url\", object), (\"bounds\", \"float64\", 4)])\n",
    "#asset_table = np.full((len(plain_items), len(asset_ids)), None, dtype=ASSET_TABLE_DT) # fills numpy array of shape (3,) filled with nan. holds scene info\n",
    "\n",
    "# if items empty, throw error\n",
    "#if len(plain_items) == 0:\n",
    "    #raise ValueError(\"No items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from functools import lru_cache\n",
    "#import pyproj\n",
    "\n",
    "#@lru_cache(maxsize=32) # this ensures we only really calc once if same epsgs\n",
    "#def cached_transform(from_epsg, to_epsg, skip_equivalent, always_xy):\n",
    "    #return pyproj.Transformer.from_crs(from_epsg, \n",
    "                                       #to_epsg, \n",
    "                                       #skip_equivalent=True, \n",
    "                                       #always_xy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def bounds_from_affine(af, ysize, xsize, from_epsg, to_epsg):   \n",
    "    \n",
    "    #ul_x, ul_y = af * (0, 0)\n",
    "    #ll_x, ll_y = af * (0, ysize)\n",
    "    #lr_x, lr_y = af * (xsize, ysize)\n",
    "    #ur_x, ur_y = af * (xsize, 0)\n",
    "\n",
    "    #xs = [ul_x, ll_x, lr_x, ur_x]\n",
    "    #ys = [ul_y, ll_y, lr_y, ur_y]\n",
    "\n",
    "    #if from_epsg != to_epsg:\n",
    "        #transformer = pyproj.Transformer.from_crs(from_epsg, \n",
    "                                                  #to_epsg, \n",
    "                                                  #skip_equivalent=True, \n",
    "                                                  #always_xy=True)\n",
    "        #transformer = cached_transform(from_epsg, \n",
    "                                       #to_epsg, \n",
    "                                       #skip_equivalent=True, \n",
    "                                       #always_xy=True)\n",
    "        \n",
    "        #xs_proj, ys_proj = transformer.transform(xs, ys, errcheck=True)\n",
    "    #else:\n",
    "        #xs_proj = xs\n",
    "        #ys_proj = ys\n",
    "\n",
    "    #return min(xs_proj), min(ys_proj), max(xs_proj), max(ys_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def snapped_bounds(bounds, resolutions_xy):\n",
    "    #import math\n",
    "    \n",
    "    #minx, miny, maxx, maxy = bounds\n",
    "    #xres, yres = resolutions_xy\n",
    "\n",
    "    #minx = math.floor(minx / xres) * xres\n",
    "    #maxx = math.ceil(maxx / xres) * xres\n",
    "    #miny = math.floor(miny / yres) * yres\n",
    "    #maxy = math.ceil(maxy / yres) * yres\n",
    "\n",
    "    #return (minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start working items\n",
    "#for item_i, item in enumerate(plain_items):\n",
    "    #item_epsg = item['properties'].get(\"proj:epsg\")\n",
    "    #item_bbox = item['properties'].get(\"proj:bbox\")\n",
    "    #item_shape = item['properties'].get(\"proj:shape\")\n",
    "    #item_transform = item['properties'].get(\"proj:transform\")\n",
    "    \n",
    "    #item_bbox_proj = None\n",
    "    #for asset_i, a_id in enumerate(asset_ids):\n",
    "        #try:\n",
    "            #asset = item['assets'].get(a_id)\n",
    "        #except KeyError:\n",
    "            #continue\n",
    "            \n",
    "        #asset_epsg = asset.get(\"proj:epsg\", item_epsg)\n",
    "        #asset_bbox = asset.get(\"proj:bbox\", item_bbox)\n",
    "        #asset_shape = asset.get(\"proj:shape\", item_shape)\n",
    "        #asset_transform = asset.get(\"proj:transform\", item_transform)\n",
    "        #asset_affine = None\n",
    "        \n",
    "        # stackstac has logic to use scene crs if projection not provided. we always want to project, so ignore\n",
    "        #out_epsg = int(out_epsg)\n",
    "        \n",
    "        # project bounds to requested epsg\n",
    "        #if bounds_latlon is not None and out_bounds is None:\n",
    "            \n",
    "            #from rasterio.warp import transform_bounds\n",
    "\n",
    "            # convert selected bounding box to epsg in output scene, liek so V\n",
    "            #l, b, r, t = bounds_latlon[0], bounds_latlon[1], bounds_latlon[2], bounds_latlon[3]\n",
    "            #out_bounds = bounds = transform_bounds(src_crs=4326, \n",
    "                                                   #dst_crs=out_epsg, \n",
    "                                                   #left=l, bottom=b, right=r, top=t)\n",
    "        \n",
    "        # if asset bbox exists, use that, else use scene bbox\n",
    "        # not doing that\n",
    "        # use asset transform instead\n",
    "        #import affine\n",
    "        #if asset_transform is not None and asset_shape is not None and asset_epsg is not None:\n",
    "            #asset_affine = affine.Affine(*asset_transform[:6]) # get affine\n",
    "            \n",
    "            # check the lru_cache thing here in stackstac, might go faster\n",
    "            #asset_bbox_proj = bounds_from_affine(asset_affine,\n",
    "                                                 #asset_shape[0],\n",
    "                                                 #asset_shape[1],\n",
    "                                                 #asset_epsg,\n",
    "                                                 #out_epsg)\n",
    "            \n",
    "        #else:\n",
    "            #raise ValueError('No scene transform')\n",
    "        \n",
    "        # create bounds. simplified this\n",
    "        #if bounds is None:\n",
    "            \n",
    "            #if asset_bbox_proj is None:\n",
    "                #raise ValueError('not enough spatial info')\n",
    "            \n",
    "            #if out_bounds is None:\n",
    "                #out_bounds = asset_bbox_proj\n",
    "            #else:\n",
    "                #print('code up geom_utils.union_bounds(asset_bbox_proj, out_bounds)')\n",
    "                \n",
    "        #else:\n",
    "            #if asset_bbox_proj is not None and not bounds_overlap(asset_bbox_proj, bounds):\n",
    "                #continue\n",
    "                \n",
    "        # do resolution\n",
    "        #if resolution is None:\n",
    "            #print('do some work for resolution user not provided')\n",
    "            \n",
    "        # store information\n",
    "        # creates row in array that has 1 row per scene, n columns per requested band where (url, [l, b, r, t])\n",
    "        #href = asset[\"href\"].replace('s3://dea-public-data', 'https://data.dea.ga.gov.au')\n",
    "        #asset_table[item_i, asset_i] = (href, asset_bbox_proj)\n",
    "        \n",
    "#print('Done!')\n",
    "\n",
    "# now, move things over to new vars\n",
    "# he casts out_bounds to a bbox object, which is just a tuple of l, r, t, b or whatever\n",
    "# does same for out_resolutions_xy\n",
    "# out_epsg also same, converted to int\n",
    "\n",
    "# snap bounds?\n",
    "#if snap_bounds:\n",
    "    #out_bounds = snapped_bounds(out_bounds, out_resolutions_xy)\n",
    "\n",
    "# converts values to a object\n",
    "#spec = RasterSpec(\n",
    "        #epsg=out_epsg,\n",
    "        #bounds=out_bounds,\n",
    "        #resolutions_xy=out_resolutions_xy,)\n",
    "        \n",
    "# prepare spec dictionary\n",
    "#trans = affine.Affine(out_resolutions_xy[0],   # xscale\n",
    "                      #0.0,\n",
    "                      #out_bounds[0],  # xoff\n",
    "                      #0.0,\n",
    "                      #-out_resolutions_xy[1],  # yscale\n",
    "                      #out_bounds[3],  # yoff\n",
    "                     #)\n",
    "\n",
    "\n",
    "#def get_shape(out_bounds, out_resolutions_xy):\n",
    "    #minx, miny, maxx, maxy = out_bounds\n",
    "    #xres, yres = out_resolutions_xy\n",
    "    #width = int((maxx - minx + (xres / 2)) / xres)\n",
    "    #height = int((maxy - miny + (yres / 2)) / yres)\n",
    "\n",
    "    #return (height, width)    \n",
    "    \n",
    "\n",
    "    # This is how GDAL rounds/snaps the calculation, so we do it too\n",
    "    # https://github.com/OSGeo/gdal/blob/00615775bff0681a7fbce17eb187dcfc0e000c15/gdal/apps/gdalwarp_lib.cpp#L3394-L3399\n",
    "    # (it's not quite the same as `round`)\n",
    "    #width = int((maxx - minx + (xres / 2)) / xres)\n",
    "    #height = int((maxy - miny + (yres / 2)) / yres)\n",
    "\n",
    "    #return (height, width)\n",
    "\n",
    "\n",
    "#spec = {'epsg': out_epsg,\n",
    "        #'bounds': out_bounds,\n",
    "        #'resolutions_xy': out_resolutions_xy,\n",
    "        #'shape': get_shape(out_bounds, out_resolutions_xy),\n",
    "        #'transform': trans,\n",
    "        #'vrt_params': {\n",
    "            #'crs': out_epsg,\n",
    "            #'transform': trans,\n",
    "            #'height': get_shape(out_bounds, out_resolutions_xy)[0],\n",
    "            #'width': get_shape(out_bounds, out_resolutions_xy)[1]\n",
    "        #}\n",
    "       #}        \n",
    "\n",
    "# drop items/assets where must be skipped. either asset missed, or out of bounds\n",
    "# same size as table\n",
    "#isnan_table = np.isnan(asset_table[\"bounds\"]).all(axis=-1) # uses bounds because other object isnan doesnt work\n",
    "#item_isnan = isnan_table.all(axis=1)  # any items all empty?\n",
    "#asset_id_isnan = isnan_table.all(axis=0) # bands assets all empty?\n",
    "\n",
    "# remove nan items... np.ix_ chooses any row and column where not nan\n",
    "#if item_isnan.any() or asset_id_isnan.any():\n",
    "    #asset_table = asset_table[np.ix_(~item_isnan, ~asset_id_isnan)]\n",
    "    #asset_ids = [id for id, isnan in zip(asset_ids, asset_id_isnan) if not isnan]\n",
    "    #items = [item for item, isnan in zip(items, item_isnan) if not isnan]\n",
    "    \n",
    "# returns \n",
    "#return asset_table, spec, asset_ids, plain_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITEMS TO DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO REWORK THIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.enums import Resampling\n",
    "\n",
    "arr = items_to_dask(asset_table=asset_table,\n",
    "                    spec=spec,\n",
    "                    chunksize=512*2,\n",
    "                    dtype=np.dtype('int16'),\n",
    "                    resampling=Resampling.nearest,\n",
    "                    fill_value=-999,\n",
    "                    rescale=True,\n",
    "                    reader=None,\n",
    "                    gdal_env=None,\n",
    "                    errors_as_nodata=()\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO COORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from cog import to_coords, to_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.DataArray(\n",
    "    arr,\n",
    "    *to_coords(\n",
    "        plain_items,\n",
    "        asset_ids,\n",
    "        spec,\n",
    "        xy_coords='topleft',\n",
    "        properties=None,\n",
    "        band_coords=True\n",
    "    ),\n",
    "    attrs=to_attrs(spec),\n",
    "    name=\"stackstac-\" + dask.base.tokenize(arr)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.diagnostics import ProgressBar\n",
    "#ProgressBar().register()\n",
    "\n",
    "%time ds = dataset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, map func to open and warp each chunk. this will fail if we compute based on github!\n",
    "def asset_read_and_window(asset_entry, spec, resampling):\n",
    "    \n",
    "    # to_array adds an extra element, exclude it\n",
    "    asset_entry = asset_entry[0, 0]   \n",
    "    url = asset_entry['url']\n",
    "    if url is None:\n",
    "        return None    \n",
    "    \n",
    "    # get bbox and window\n",
    "    asset_window = windows.from_bounds(*asset_entry['bounds'], \n",
    "                                       transform=spec.get('transform'))\n",
    "        \n",
    "    # open \n",
    "    open_env = rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\", VSI_CACHE=True)\n",
    "    with open_env:\n",
    "        try:\n",
    "            # TODO TEMP: need to compute for testing\n",
    "            ds = SelfCleaningDatasetReader(rasterio.parse_path(url.compute()), sharing=False)\n",
    "            #ds = rasterio.DatasetReader(rasterio.parse_path(url), sharing=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError('issue opening url at dask')\n",
    "            \n",
    "        if ds.count != 1:\n",
    "            ds.close()\n",
    "            raise RuntimeError('asset doesnt support > 1 band')\n",
    "\n",
    "        # only make a VRT if the dataset doesn't match the spatial spec we want\n",
    "        ds_meta = {\n",
    "            'crs': ds.crs.to_epsg(),\n",
    "            'transform': ds.transform,\n",
    "            'height': ds.height,\n",
    "            'width': ds.width}\n",
    "            \n",
    "        if spec.get('vrt_params') != ds_meta:\n",
    "            with open_env:\n",
    "                vrt = WarpedVRT(\n",
    "                    ds,\n",
    "                    sharing=False,\n",
    "                    resampling=resampling,\n",
    "                    **spec.get('vrt_params'))\n",
    "        else:\n",
    "            print('skip vrt')\n",
    "            vrt = None\n",
    "\n",
    "    # see githib for what this does and why they do it\n",
    "    if ds.driver in ['GTiff']:\n",
    "        scale_offset = (ds.scales[0], ds.offsets[0])\n",
    "        \n",
    "        if vrt is not None:\n",
    "            vrt_params = {\n",
    "                'crs': vrt.crs.to_string(),\n",
    "                'resampling': vrt.resampling,\n",
    "                'tolerance': vrt.tolerance,\n",
    "                'src_nodata': vrt.src_nodata,\n",
    "                'nodata': vrt.nodata,\n",
    "                'width': vrt.width,\n",
    "                'height': vrt.height,\n",
    "                'src_transform': vrt.src_transform,\n",
    "                'transform': vrt.transform,\n",
    "                'dtype': vrt.working_dtype,\n",
    "                'warp_extras': vrt.warp_extras\n",
    "            }    \n",
    "            \n",
    "        else:\n",
    "            vrt_params = None\n",
    "            \n",
    "    # threading\n",
    "    threadlocal = threading.local()\n",
    "    threadlocal.ds = ds\n",
    "    threadlocal.vrt = vrt\n",
    "   \n",
    "    \n",
    "    # return tuple of reader, asset_window\n",
    "    \n",
    "asset_read_and_window(asset_table_dask, spec, resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for threadlocalriodataset\n",
    "class ThreadLocalRioDataset:\n",
    "        \n",
    "    def __init__(self, env, ds, vrt):\n",
    "        #self._env = env # this takes layeredenv object, work around it\n",
    "        self._url = ds.name\n",
    "        self._driver = ds.driver\n",
    "        self._open_options = ds.options\n",
    "        \n",
    "        # Cache this for non-locking access\n",
    "        self.scale_offset = (ds.scales[0], ds.offsets[0])\n",
    "        \n",
    "        if vrt is not None:\n",
    "            self._vrt_params = dict(\n",
    "                # src_crs=vrt.src_crs.to_string(),\n",
    "                # ^ we won't use this, and loading proj4 CRSs is slow\n",
    "                crs=vrt.crs.to_string(),\n",
    "                # ^ we _do_ ser-de the CRS to re-create it per thread,\n",
    "                # because pyproj.CRS objects probably aren't thread-safe?\n",
    "                resampling=vrt.resampling,\n",
    "                tolerance=vrt.tolerance,\n",
    "                src_nodata=vrt.src_nodata,\n",
    "                nodata=vrt.nodata,\n",
    "                width=vrt.width,\n",
    "                height=vrt.height,\n",
    "                src_transform=vrt.src_transform,\n",
    "                transform=vrt.transform,\n",
    "                dtype=vrt.working_dtype,\n",
    "                warp_extras=vrt.warp_extras,\n",
    "            )\n",
    "            # ^ copied from rioxarray\n",
    "            # https://github.com/corteva/rioxarray/blob/0804791a44f65ac4f303dd286e94b3eaee81f72b/rioxarray/_io.py#L720-L734\n",
    "        else:\n",
    "            self._vrt_params = None\n",
    "            \n",
    "        self._threadlocal = threading.local()\n",
    "        self._threadlocal.ds = ds\n",
    "        self._threadlocal.vrt = vrt\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "    def _open(self):\n",
    "        # see git for help with env handling\n",
    "        with rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\", VSI_CACHE=True):\n",
    "            \n",
    "            # this is weird\n",
    "            result = ds = rasterio.DatasetReader(rasterio.parse_path(url), \n",
    "                                                 sharing=False,\n",
    "                                                 driver=self._driver,\n",
    "                                                 **self._open_options)\n",
    "            \n",
    "            if self._vrt_params:\n",
    "                with rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\", VSI_CACHE=True):\n",
    "                    result = vrt = WarpedVRT(ds, sharing=False, **self._vrt_params)\n",
    "            else:\n",
    "                vrt = None\n",
    "        \n",
    "        with self._lock:\n",
    "            self._threadlocal.ds = ds\n",
    "            self._threadlocal.vrt = vrt\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def dataset(self):\n",
    "        try:\n",
    "            with self._lock:\n",
    "                return self._threadlocal.vrt or self._threadlocal.ds\n",
    "        except AttributeError:\n",
    "            return self._open()\n",
    "\n",
    "    def read(self, window, **kwargs):\n",
    "        \"Read from the current thread's dataset, opening a new copy of the dataset on first access from each thread.\"\n",
    "        with rasterio.Env(VSI_CACHE=False):\n",
    "            return self.dataset.read(1, window=window, **kwargs)\n",
    "        \n",
    "    def close(self):\n",
    "        with self._lock:\n",
    "            self._threadlocal = threading.local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each block above ^ ^ ^ this is what hapopens\n",
    "# autoparallelrioreadert is called\n",
    "# url is sent through\n",
    "# gdal driver checked for pmulti or single thread?\n",
    "def AutoParallelRioReader(url, spec, resampling):\n",
    "    allowed_driver = ['GTiff']\n",
    "    \n",
    "    print(url)\n",
    "\n",
    "    #url, bounds = asset_table_dask.compute()[0][0]\n",
    "    #url = url.replace('s3://dea-public-data', 'https://data.dea.ga.gov.au')\n",
    "\n",
    "    # open with gdal env\n",
    "    with rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\", VSI_CACHE=True):\n",
    "        try:\n",
    "            ds = rasterio.DatasetReader(rasterio.parse_path(url), sharing=False)\n",
    "        except Exception as e:\n",
    "            # use nodatareader here to write nodata see https://github.com/gjoseph92/stackstac/blob/5f984b211993380955b5d3f9eba3f3e285f6952c/stackstac/rio_reader.py#L311\n",
    "            raise ValueError('issue opening url at dask')\n",
    "\n",
    "        if ds.count != 1:\n",
    "            ds.close()\n",
    "            raise RuntimeError('asset doesnt support > 1 band')\n",
    "\n",
    "        # only make a VRT if the dataset doesn't match the spatial spec we want\n",
    "        ds_meta = {\n",
    "            'crs': ds.crs.to_epsg(),\n",
    "            'transform': ds.transform,\n",
    "            'height': ds.height,\n",
    "            'width': ds.width\n",
    "        }\n",
    "        if spec.get('vrt_params') != ds_meta:\n",
    "\n",
    "            # look at git for gdal options, they used open_vrt option. how it differ from above?\n",
    "            with rasterio.Env(GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\", VSI_CACHE=True):\n",
    "                vrt = WarpedVRT(\n",
    "                    ds,\n",
    "                    sharing=False,\n",
    "                    resampling=resampling,\n",
    "                    **spec.get('vrt_params')\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print('skip vrt')\n",
    "            vrt = None\n",
    "\n",
    "\n",
    "    # see githib for what this does and why they do it\n",
    "    if ds.driver in allowed_driver:\n",
    "        1\n",
    "        # returns threadlocalriodataset\n",
    "        # lets do it here to understand it\n",
    "\n",
    "        # threadhlocalriodataset\n",
    "        reader = ThreadLocalRioDataset(None, ds, vrt=vrt)\n",
    "        print(reader)\n",
    "        #return ThreadLocalRioDataset(self.gdal_env, ds, vrt=vrt)\n",
    "\n",
    "    else:\n",
    "        raise('lewie not supported yet, see singlethreaded riodataset method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is the main make dask func V V V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import windows\n",
    "def asset_entry_to_reader_and_window(asset_entry, spec, resampling, dtype, fill_value, rescale):\n",
    "    \n",
    "    # dask adds extra outer dims in from_array, so we take specific\n",
    "    asset_entry = asset_entry[0, 0] # basically, takes url, bounding box for first band... does it do all three?!\n",
    "    url = asset_entry[\"url\"]\n",
    "    if url is None:\n",
    "        return None\n",
    "\n",
    "    asset_bounds = asset_entry[\"bounds\"]\n",
    "    asset_window = windows.from_bounds(*asset_bounds, transform=spec.get('transform'))\n",
    "    \n",
    "    # see stackstac comment for explkanations\n",
    "    return AutoParallelRioReader(url, spec, resampling), asset_window\n",
    "\n",
    "    \n",
    "   # return (\n",
    "        #reader(\n",
    "            #url=url,\n",
    "            #spec=spec,\n",
    "            #resampling=resampling,\n",
    "            #dtype=dtype,\n",
    "            #fill_value=fill_value,\n",
    "            #rescale=rescale,\n",
    "            #gdal_env=gdal_env,\n",
    "            #errors_as_nodata=errors_as_nodata,\n",
    "        #),\n",
    "        #asset_window,\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin doing items_to_dask func\n",
    "# we need asset_table, spec, chunksize, dtype, resampling\n",
    "# fill_value, rescale, reader, gdal_env, errors_as_nodata\n",
    "\n",
    "if fill_value is None and errors_as_nodata:\n",
    "    raise ValueError('Must provide a errors as nodata value if fill_value provided')\n",
    "    \n",
    "# gets a list of exceptions such as http response code 404... really weird stuff\\\n",
    "# todo\n",
    "#errors_as_nodata = errors_as_nodata or ()\n",
    "\n",
    "if fill_value is not None and not np.can_cast(fill_value, dtype):\n",
    "    raise ValueError('Fill value incompatible with output type.')\n",
    "    \n",
    "# make urls into dask array with 1-element chunks (i.e. 1 chunk per asset (i.e.e band))\n",
    "asset_table_dask = da.from_array(asset_table, \n",
    "                                 chunks=1, \n",
    "                                 #inline_array=True, doesnt work on this version\n",
    "                                 name='asset-table-' + dask.base.tokenize(asset_table))\n",
    "\n",
    "\n",
    "\n",
    "# now, map a function over each chunk\n",
    "# the func opens url as rasterio dataset\n",
    "# see stackstac comment for info here\n",
    "# So now we have an array of shape (items, assets), chunksize 1---the outer two dimensions of our final array.\n",
    "datasets = asset_table_dask.map_blocks(asset_entry_to_reader_and_window,\n",
    "                                       spec,\n",
    "                                       resampling,\n",
    "                                       dtype,\n",
    "                                       fill_value,\n",
    "                                       rescale,\n",
    "                                       #gdal_env, # todo\n",
    "                                       #errors_as_nodata, # todo\n",
    "                                       #reader, # todo\n",
    "                                       meta=asset_table_dask._meta)\n",
    "\n",
    "# continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# move forward\n",
    "shape = spec.get('shape')\n",
    "name = \"slices-\" + dask.base.tokenize(chunksize, shape)\n",
    "chunks = da.core.normalize_chunks(chunksize, shape)\n",
    "keys = itertools.product([name], *(range(len(bds)) for bds in chunks))\n",
    "slices = da.core.slices_from_chunks(chunks)\n",
    "\n",
    "slices_fake_arr = da.Array(\n",
    "    dict(zip(keys, slices)), name, chunks, meta=datasets._meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
