{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise COG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/geopandas/_compat.py:110: UserWarning: The Shapely GEOS version (3.7.2-CAPI-1.11.0 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  shapely_geos_version, geos_capi_version_string\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import html\n",
    "import requests\n",
    "import gdal\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "sys.path.append('../../../Scripts')\n",
    "from dea_dask import create_local_dask_cluster\n",
    "\n",
    "sys.path.append('../../shared')\n",
    "import satfetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45529</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/lewis/proxy/8787/status' target='_blank'>/user/lewis/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>13.11 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:45529' processes=1 threads=2, memory=13.11 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialise the cluster\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get study area polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load study area geometry as geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read study area and get bounds as a list\n",
    "gdf = gpd.read_file('../../data/cog/yandisa.geojson')\n",
    "gdf_bounds = gdf.bounds.values[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set STAC Search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get satellite collection on dea. todo get from user in arcgis, sentinel 2\n",
    "collections = [\n",
    "    'ga_ls5t_ard_3', \n",
    "    'ga_ls7e_ard_3',\n",
    "    'ga_ls8c_ard_3'\n",
    "]\n",
    "\n",
    "# set required bands\n",
    "bands = [\n",
    "    'nbart_blue', \n",
    "    'nbart_green', \n",
    "    'nbart_red', \n",
    "    'nbart_nir',\n",
    "    'nbart_swir_1',\n",
    "    'nbart_swir_2',\n",
    "    'oa_fmask'\n",
    "]\n",
    "\n",
    "# get satellite collection date range, convert to stac. todo get from user in arcgis\n",
    "start_dt, end_dt = '1990-01-01', '1995-12-31'\n",
    "\n",
    "# bring it all together for a query\n",
    "query = {\n",
    "    'collections': collections,\n",
    "    'datetime': '{0}/{1}'.format(start_dt, end_dt),\n",
    "    'bbox': gdf_bounds,\n",
    "    'query': {'eo:cloud_cover': {'lt': 5}}, #this doesnt work\n",
    "    'limit': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch DEA Public Data via STAC Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146 satellite scenes in total.\n"
     ]
    }
   ],
   "source": [
    "# set stac endpoint\n",
    "search_endpoint = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'\n",
    "\n",
    "# send and get request for stac json using \n",
    "stac_response = requests.post(search_endpoint, json=query)\n",
    "\n",
    "# check for response empty errors, convert to json if so\n",
    "if stac_response.ok:\n",
    "    stac_response = stac_response.json()\n",
    "    num_items = len(stac_response.get('features'))\n",
    "    print('Found {0} satellite scenes in total.'.format(num_items))\n",
    "else:\n",
    "    raise ValueError('Could not connect to DEA STAC SEARCH endpoint.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate STAC response and remove cloud cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 23 satellite scenes due to clouds.\n",
      "Total of 123 satellite scenes remaining.\n"
     ]
    }
   ],
   "source": [
    "# set max cloud cover (0 - 100)\n",
    "max_cloud = 25\n",
    "\n",
    "# get num of all stac scenes\n",
    "num_all_items = len(stac_response.get('features'))\n",
    "\n",
    "feat_list = []\n",
    "for feat in stac_response.get('features'):\n",
    "    if max_cloud > float(feat.get('properties').get('eo:cloud_cover')):\n",
    "        feat_list.append(feat)\n",
    "        \n",
    "# count cloud less scenes and compare\n",
    "if feat_list:\n",
    "    num_clean_items = len(feat_list)\n",
    "    print('Removed {0} satellite scenes due to clouds.'.format(num_all_items - num_clean_items))\n",
    "    print('Total of {0} satellite scenes remaining.'.format(num_clean_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build VRTs for each band of each scene in STAC response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting landsat vrts for each relevant bands.\n",
      "Building landsat vrt list for band: nbart_blue.\n",
      "Building landsat vrt list for band: nbart_green.\n",
      "Building landsat vrt list for band: nbart_red.\n",
      "Building landsat vrt list for band: nbart_nir.\n",
      "Building landsat vrt list for band: nbart_swir_1.\n",
      "Building landsat vrt list for band: nbart_swir_2.\n",
      "Building landsat vrt list for band: oa_fmask.\n",
      "Got 7 landsat vrt band lists successfully.\n"
     ]
    }
   ],
   "source": [
    "# get dict of band names and associated vrt lists\n",
    "band_vrt_dict = satfetcher.get_dea_landsat_vrt_dict(feat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine VRTs into file for each band set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining VRTs into single VRTs per band.\n",
      "Combining VRTs into temp. file for band: nbart_blue.\n",
      "Combining VRTs into temp. file for band: nbart_green.\n",
      "Combining VRTs into temp. file for band: nbart_red.\n",
      "Combining VRTs into temp. file for band: nbart_nir.\n",
      "Combining VRTs into temp. file for band: nbart_swir_1.\n",
      "Combining VRTs into temp. file for band: nbart_swir_2.\n",
      "Combining VRTs into temp. file for band: oa_fmask.\n",
      "Combined 7 band vrt lists successfully.\n"
     ]
    }
   ],
   "source": [
    "# loop each band and combine vrts into one per band\n",
    "vrt_file_dict = satfetcher.combine_vrts_per_band(band_vrt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract datetimes from vrt file for each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datetimes for VRTs per band.\n",
      "Extracting datetimes from VRTs for band: nbart_blue.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_green.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_red.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_nir.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_swir_1.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: nbart_swir_2.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracting datetimes from VRTs for band: oa_fmask.\n",
      "Warning: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1) at index: 1.\n",
      "Extracted 7 band vrt datetimes successfully.\n"
     ]
    }
   ],
   "source": [
    "# loop each band and extract datetimes for each\n",
    "vrt_dt_dict = satfetcher.get_vrt_file_datetimes(vrt_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks, meta\n",
    "def prepare_full_vrt_dicts(vrt_file_dict, vrt_dt_dict):\n",
    "    \"\"\"\n",
    "    takes vrt file and datetime file dicts and combines\n",
    "    into one final dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # imports\n",
    "    from collections import Counter\n",
    "    \n",
    "    # checks\n",
    "    \n",
    "    # get list of band names in dict\n",
    "    file_bands = [band for band in vrt_file_dict]\n",
    "    dt_bands = [band for band in vrt_dt_dict]\n",
    "    \n",
    "    # check if same bands lists identical\n",
    "    if Counter(file_bands) != Counter(dt_bands):\n",
    "        raise ValueError('VRT and datetime band names not identical.')\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2', 'oa_fmask']\n",
      "['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2', 'oa_fmask']\n",
      "['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2', 'oa_fmask']\n"
     ]
    }
   ],
   "source": [
    "prepare_full_vrt_dicts(vrt_file_dict, vrt_dt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse datetime strings into map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vrts\n",
    "dt_blue = parse_datetimes(vrt_string=vrt_blue_out)\n",
    "dt_green = parse_datetimes(vrt_string=vrt_green_out)\n",
    "dt_red = parse_datetimes(vrt_string=vrt_red_out)\n",
    "dt_nir = parse_datetimes(vrt_string=vrt_nir_out)\n",
    "dt_swir_1 = parse_datetimes(vrt_string=vrt_swir_1_out)\n",
    "dt_swir_2 = parse_datetimes(vrt_string=vrt_swir_2_out)\n",
    "dt_mask = parse_datetimes(vrt_string=vrt_mask_out)\n",
    "\n",
    "# check if lengths are all same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xr_dataset(vrt_file, band_name):\n",
    "    \n",
    "    # setup chunks\n",
    "    chunks = {'band': 1, 'x': 'auto', 'y': 'auto'}\n",
    "    \n",
    "    # load xr as data array\n",
    "    ds = xr.open_rasterio(vrt_file, chunks=chunks)\n",
    "    \n",
    "    # rename default band label to time\n",
    "    ds = ds.rename({'band': 'time'})\n",
    "    \n",
    "    # convert to dataset\n",
    "    ds = ds.to_dataset(name=band_name, promote_attrs=True)\n",
    "    \n",
    "    # subset to coords, bb todo fix this up\n",
    "    ds = ds.isel(x=slice(4000, 5000), y=slice(3000, 4000))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "ds_blue = build_xr_dataset(vrt_file=vrt_blue_out, band_name='nbart_blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace datetime\n",
    "def replace_datetimes(ds, dt):\n",
    "    \n",
    "    # replace timezone and convert numpy\n",
    "    dt_dict = {}\n",
    "    for k, v in dt_blue.items():\n",
    "        dt_dict[k] = np.datetime64(v.replace('Z', ''))\n",
    "    \n",
    "    # remap\n",
    "    ds['time'] = [dt_dict[i] for i in ds['time'].values.tolist()]\n",
    "    return ds.sortby('time')\n",
    "    \n",
    "ds_blue = replace_datetimes(ds_blue, dt_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute\n",
    "%time ds_blue = ds_blue.compute()\n",
    "ds_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all dask datasets into one\n",
    "#xr.merge([ds_blue, ds_green, ds_red])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test download times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try raw, without dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed testing without dask distributed\n",
    "%time ds = ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try raw, without dask but with threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed testing without dask distributed\n",
    "%time ds = ds.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try raw, without dask but with processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed testing without dask distributed\n",
    "%time ds = ds.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try dask, with distributed scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=True)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 47 secs with processes=false, 21 secs when True\n",
    "%time ds = ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try dask data arrays split and futures used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures \n",
    "\n",
    "# create compute func\n",
    "def compute_da(da):\n",
    "    return da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split ds into seperate das\n",
    "da_list = []\n",
    "for dt in ds['time']:\n",
    "    da = ds.sel(time=dt)\n",
    "    da_list.append(da)\n",
    "    \n",
    "# try parallel load of all bands\n",
    "num_cores = 2\n",
    "with concurrent.futures.ThreadPoolExecutor(num_cores) as executor:\n",
    "    %time da_list = list(executor.map(compute_da, da_list))\n",
    "    \n",
    "ds = xr.concat(da_list, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this to auto gen vrt to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really good test env\n",
    "vrt_1_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/02/09/ga_ls5t_nbart_3-0-0_112076_1990-02-09_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/02/09/ga_ls5t_nbart_3-0-0_112076_1990-02-09_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/02/09/ga_ls5t_nbart_3-0-0_112076_1990-02-09_final_band03.tif']\n",
    "vrt1 = gdal.BuildVRT('vrt_1.vrt', vrt_1_urls, separate=True).FlushCache()\n",
    "\n",
    " \n",
    "vrt_2_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/13/ga_ls5t_nbart_3-0-0_112076_1990-03-13_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/13/ga_ls5t_nbart_3-0-0_112076_1990-03-13_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/13/ga_ls5t_nbart_3-0-0_112076_1990-03-13_final_band03.tif']\n",
    "vrt2 = gdal.BuildVRT('vrt_2.vrt', vrt_2_urls, separate=True).FlushCache()\n",
    "\n",
    "\n",
    "vrt_3_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/111/076/1990/03/22/ga_ls5t_nbart_3-0-0_111076_1990-03-22_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/111/076/1990/03/22/ga_ls5t_nbart_3-0-0_111076_1990-03-22_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/111/076/1990/03/22/ga_ls5t_nbart_3-0-0_111076_1990-03-22_final_band03.tif']\n",
    "vrt3 = gdal.BuildVRT('vrt_3.vrt', vrt_3_urls, separate=True).FlushCache()\n",
    "\n",
    "\n",
    "vrt_4_urls = [\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/29/ga_ls5t_nbart_3-0-0_112076_1990-03-29_final_band01.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/29/ga_ls5t_nbart_3-0-0_112076_1990-03-29_final_band02.tif',\n",
    "    '/vsicurl/https://data.dea.ga.gov.au/baseline/ga_ls5t_ard_3/112/076/1990/03/29/ga_ls5t_nbart_3-0-0_112076_1990-03-29_final_band03.tif']\n",
    "vrt4 = gdal.BuildVRT('vrt_4.vrt', vrt_4_urls, separate=True).FlushCache()\n",
    "\n",
    "# add to list\n",
    "vrt_list = ['vrt_1.vrt', 'vrt_2.vrt', 'vrt_3.vrt', 'vrt_4.vrt']\n",
    "vrt_out = gdal.BuildVRT('vrt_all.vrt', vrt_list, separate=False, bandList=[1]).FlushCache()\n",
    "\n",
    "\n",
    "# read it in to memory and decode it\n",
    "#vrt_all = tmp.read().decode(\"utf-8\")\n",
    "\n",
    "# setup chunks\n",
    "chunks = {'band': 1, 'x': 'auto', 'y': 'auto'}\n",
    "ds = xr.open_rasterio('vrt_all.vrt', chunks=chunks)\n",
    "ds = ds.isel(x=slice(2500, 3000), y=slice(2500, 3000))\n",
    "ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
