# -*- coding: utf-8 -*-# https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/a-quick-tour-of-python-toolboxes.htm# imports todo - fix this up properlyimport os, certifios.environ['GDAL_DATA']  = r'C:\Program Files\ArcGIS\Pro\Resources\pedata\gdaldata'os.environ.setdefault("CURL_CA_BUNDLE", certifi.where())# importsimport arcpy# globalsSTAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'AWS_KEY = ''AWS_SECRET = ''RESULT_LIMIT = 250class Toolbox(object):    def __init__(self):        """Define the toolbox (the name of the toolbox is the name of the        .pyt file)."""           self.label = "Toolbox"        self.alias = "toolbox"        # list of tool classes associated with this toolbox        self.tools = [            Tool,             COG_Fetch,             COG_Sync,             COG_Explore,             GDVSpectra_Likelihood,             GDVSpectra_Threshold,             GDVSpectra_Trend,            GDVSpectra_CVA,            Phenolopy_Metrics,            VegFrax_Fractional_Cover            ]class Tool(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "Tool"        self.description = ""        self.canRunInBackground = False    def getParameterInfo(self):        """Define parameter definitions"""        params = None        return params    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """The source code of the tool."""        returnclass COG_Fetch(object):    def __init__(self):              # set tool name        self.label = "COG Fetch"                # set tool description        self.description = "COG contains functions that " \                           "allow for efficient download of " \                           "analysis ready data (ARD) Landsat " \                           "5, 7, 8 or Sentinel 2A, 2B images " \                           "from the Digital Earth Australia " \                           "(DEA) public AWS server."                                   # set false for pro        self.canRunInBackground = False    def getParameterInfo(self):                # input study area shapefile        par_studyarea_feat = arcpy.Parameter(                                displayName="Input study area feature",                                name="in_studyarea_feat",                                datatype="GPFeatureLayer",                                parameterType="Required",                                direction="Input"                                )                                        # set study area to be polygon only        par_studyarea_feat.filter.list = ['Polygon']                                        # output file location        par_out_nc_path = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_path",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_path.filter.list = ['nc']        # in_platform        par_platform = arcpy.Parameter(                            displayName="Satellite platform",                            name="in_platform",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default platform        par_platform.values = 'Landsat'        par_platform.filter.list = ['Landsat', 'Sentinel']                # in_from_date        par_date_from = arcpy.Parameter(                            displayName="Date from",                            name="in_from_date",                            datatype="GPDate",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set in_from_date value        par_date_from.values = '2015/01/01'                # in_to_date        par_date_to = arcpy.Parameter(                        displayName="Date to",                        name="in_to_date",                        datatype="GPDate",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )        # set in_from_date value        par_date_to.values = '2020/12/31'        # set bands        par_bands = arcpy.Parameter(                        displayName="Bands",                        name="in_bands",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        category='Satellite Bands',                        multiValue=True                        )                         # set landsat bands        bands = [            'Blue',             'Green',             'Red',             'NIR',             'SWIR1',             'SWIR2',             'OA_Mask'            ]                # set default bands        par_bands.filter.type = "ValueList"                par_bands.filter.list = bands        par_bands.values = bands                # set slc-off        par_slc_off = arcpy.Parameter(                        displayName="SLC Off",                        name="in_slc_off",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                # set slc-off value        par_slc_off.value = False                       # set output datatype        par_output_dtype = arcpy.Parameter(                            displayName="Output data type",                            name="in_output_dtype",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_dtype.filter.list = ['int8', 'int16', 'float32', 'float64']        par_output_dtype.values = 'int16'                # todo make this changeh when sent/landsat changed        # set output resolution        par_output_res = arcpy.Parameter(                            displayName="Output pixel resolution",                            name="in_output_res",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default platform        par_output_res.filter.type = 'Range'        par_output_res.filter.list = [0, 1000]        par_output_res.value = 30         # todo allow this to handle np.nan        # set output nodata value        par_output_fill_value = arcpy.Parameter(                                    displayName="Output NoData value",                                    name="in_output_fill_value",                                    datatype="GPString",                                    parameterType="Required",                                    direction="Input",                                    category='Warping Options',                                    multiValue=False                                    )                                    # set default nodata value        par_output_fill_value.value = "-999"                # set output epsg        par_output_epsg = arcpy.Parameter(                            displayName="Output EPSG",                            name="in_output_epsg",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default epsg        par_output_epsg.filter.list = [3577]        par_output_epsg.values = 3577                # set resampling type        par_output_resampling = arcpy.Parameter(                            displayName="Resampling type",                            name="in_output_resampling",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default resampling        par_output_resampling.filter.list = ['Nearest', 'Bilinear']        par_output_resampling.values = 'Nearest'                # set snap boundary        par_output_snap = arcpy.Parameter(                            displayName="Snap boundaries",                            name="in_snap_bounds",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                # set snap boundary value        par_output_snap.value = True                # set rescale        par_output_rescale = arcpy.Parameter(                        displayName="Rescale",                        name="in_rescale",                        datatype="GPBoolean",                        parameterType="Required",                        direction="Input",                        category='Warping Options',                        multiValue=False                        )                # set rescale value        par_output_rescale.value = True                # set cell alignment        par_output_cell_align = arcpy.Parameter(                            displayName="Cell alignment",                            name="in_output_cell_align",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            category='Warping Options',                            multiValue=False                            )                                    # set default cell align        par_output_cell_align.filter.list = ['Top-left', 'Center']        par_output_cell_align.values = 'Top-left'                # set chunks        par_output_chunk_size = arcpy.Parameter(                            displayName="Chunk size",                            name="in_output_chunk_size",                            datatype="GPLong",                            parameterType="Required",                            direction="Input",                            category='Parallelisation',                            multiValue=False                            )                                    # set default chunksize        par_output_chunk_size.value = -1                # set dea aws stac url        par_output_stac_url = arcpy.Parameter(                                displayName="Digital Earth Australia STAC URL",                                name="in_output_stac_url",                                datatype="GPString",                                parameterType="Required",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws stac url        par_output_stac_url.value = STAC_ENDPOINT        # set dea aws key        par_output_aws_key = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Key",                                name="in_output_aws_key",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws key value        par_output_aws_key.value = AWS_KEY         # set dea aws secret        par_output_aws_secret = arcpy.Parameter(                                displayName="Digital Earth Australia AWS Secret Key",                                name="in_output_aws_secret",                                datatype="GPString",                                parameterType="Optional",                                direction="Input",                                category='STAC Options',                                multiValue=False                                )                 # set default dea aws secret value        par_output_aws_secret.value = AWS_SECRET                # combine parameters        parameters = [            par_studyarea_feat,            par_out_nc_path,            par_platform,            par_date_from,            par_date_to,            par_bands,            par_slc_off,            par_output_dtype,            par_output_res,            par_output_fill_value,            par_output_epsg,            par_output_resampling,            par_output_snap,            par_output_rescale,            par_output_cell_align,            par_output_chunk_size,            par_output_stac_url,            par_output_aws_key,            par_output_aws_secret        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        """                # update ui when platform changed        if parameters[2].value == 'Landsat':                    # enable slc-off control            parameters[6].enabled = True                        # update landsat band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR',                 'SWIR1',                 'SWIR2',                 'OA_Mask'                ]                        # update values in control            parameters[5].filter.type = 'ValueList'            parameters[5].filter.list = bands            parameters[5].values = bands                        # set resolution to original 30x30m            parameters[8].value = 30        else:                    # disable slc-off control            parameters[6].enabled = False                        # update sentinel band list            bands = [                'Blue',                 'Green',                 'Red',                 'NIR1',                 'SWIR2',                 'SWIR3',                 'OA_Mask'                ]                        # update values in control            parameters[5].filter.type = 'ValueList'            parameters[5].filter.list = bands            parameters[5].values = bands                        # set resolution to original 10x10m            parameters[8].value = 10        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        """                        # imports        import os, sys        import io        import time        import numpy as np        import xarray as xr        import dask        import dask.array as da        import arcpy        # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, tools, satfetcher                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import cog                # notify         arcpy.AddMessage('Performing COG Fetch.')                                                    # grab parameter values         in_studyarea_feat = parameters[0].value         # study area feat         out_nc = parameters[1].valueAsText              # output nc         in_platform = parameters[2].value               # platform name        in_from_date = parameters[3].value              # from date        in_to_date = parameters[4].value                # to date        in_bands = parameters[5].valueAsText            # bands        in_slc_off = parameters[6].value                # slc off         in_dtype = parameters[7].value                  # output pixel dtype        in_res = parameters[8].value                    # output pixel resolution        in_fill_value = parameters[9].value             # todo processing string to int, float or np.nan        in_epsg = parameters[10].value                  # output epsg         in_resampling = parameters[11].value            # output resampler method         in_snap = parameters[12].value                  # output snap alignment         in_rescale = parameters[13].value               # output rescale        in_cell_align = parameters[14].value            # output cell alignmnent             in_chunk_size = parameters[15].value            # chunk size        in_stac_endpoint = parameters[16].value         # stac endpoint        in_aws_key = parameters[17].value               # dea aws key        in_aws_secret = parameters[18].value            # dea aws secret                # let user know that aws key and secret not yet implemented        if in_aws_key is not None or in_aws_secret is not None:            arcpy.AddWarning('AWS Credentials not yet supported. Using DEA AWS.')                # set up progess bar        arcpy.SetProgressor(type='default', message='Preparing query parameters...')                # get minimum bounding geom from input         bbox = arc.get_selected_layer_extent(in_studyarea_feat)                # get collections based on platform         collections = arc.prepare_collections_list(in_platform)                    # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # fetch stac data         arcpy.SetProgressorLabel('Performing STAC query...')        feats = cog.fetch_stac_data(stac_endpoint=in_stac_endpoint,                                     collections=collections,                                     start_dt=in_from_date,                                     end_dt=in_to_date,                                     bbox=bbox,                                    slc_off=in_slc_off,                                    limit=RESULT_LIMIT)                # count number of items        arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))        # prepare band (i.e. stac assets) names        assets = arc.prepare_band_names(in_bands=in_bands,                                         in_platform=in_platform)                                                            # convert raw stac into dict with coord reproject, etc.        arcpy.SetProgressorLabel('Converting STAC data into useable format...')        meta, asset_table = cog.prepare_data(feats,                                              assets=assets,                                             bounds_latlon=bbox,                                              bounds=None,                                              epsg=in_epsg,                                              resolution=in_res,                                              snap_bounds=in_snap,                                             force_dea_http=True)                                                     # prepare resample and fill value types        resampling = in_resampling.lower()        fill_value = arc.prepare_fill_value_type(in_fill_value)                                                                                                  # convert assets to dask array        arcpy.SetProgressorLabel('Parallelising data...')        darray = cog.convert_to_dask(meta=meta,                                      asset_table=asset_table,                                      chunksize=in_chunk_size,                                     resampling=resampling,                                      dtype=in_dtype,                                      fill_value=fill_value,                                      rescale=in_rescale)                                             # prepare alignment type        cell_align = arc.prepare_cell_align_type(in_cell_align)        # generate coordinates and dimensions from metadata        arcpy.SetProgressorLabel('Building dataset metadata...')        coords, dims = cog.build_coords(feats=feats,                                        assets=assets,                                         meta=meta,                                        pix_loc=cell_align)                # build final xarray data array        arcpy.SetProgressorLabel('Finalising dataset...')        ds_name = 'stac-' + dask.base.tokenize(darray)        ds = xr.DataArray(darray,                          coords=coords,                          dims=dims,                          name=ds_name                          )                                 # comvert to cleaner xarray dataset        ds = ds.to_dataset(dim='band')                # append attributes onto dataset        ds = cog.build_attributes(ds=ds,                                  meta=meta,                                   collections=collections,                                   bands=assets,                                  slc_off=in_slc_off,                                   bbox=bbox,                                  dtype=in_dtype,                                  snap_bounds=in_snap,                                  fill_value=fill_value,                                   rescale=in_rescale,                                  cell_align=in_cell_align,                                  resampling=in_resampling)                                             # set up proper progress bar        arcpy.SetProgressor(type='step',                             message='Preparing data download...',                             min_range=0,                             max_range=len(ds.data_vars) + 1)        # get list of dataset vars and iterate compute on each        for counter, data_var in enumerate(list(ds.data_vars), start=1):                    # start clock            start = time.time()                    # update progress bar            arcpy.SetProgressorLabel('Downloading band: {}...'.format(data_var))            arcpy.SetProgressorPosition(counter)                    # compute!            ds[data_var] = ds[data_var].compute()                        # notify time             duration = round(time.time() - start, 2)            arcpy.AddMessage('Band: {} took: {}s to download.'.format(data_var, duration))                                          # wrap up         arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(counter + 1)                        # export netcdf to output folder        tools.export_xr_as_nc(ds=ds, filename=out_nc)        # notify finish        arcpy.AddMessage('COG Fetch completed successfully.')                returnclass COG_Sync(object):    def __init__(self):            # set tool name        self.label = "COG Sync"                # set tool description        self.description = "Sync COG to update cube with latest " \                           "data."                                   # set false for pro        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']                # combine parameters        parameters = [            par_nc_file            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                # imports        import os, sys        #import io        #import time        import pandas as pd        import numpy as np        import xarray as xr        #from tempfile import NamedTemporaryFile        import arcpy        # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, tools, satfetcher                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import cog                # globals         AWS_KEY = ''        AWS_SECRET = ''        STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'        RESULT_LIMIT = 250                # notify         arcpy.AddMessage('Performing COG Sync.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText      # raw netcdf        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)  #nodatavals?                # checks        if 'time' not in ds:            arcpy.AddError('No time dimension detected.')                #tod other checks                 # get original query attributes         arcpy.SetProgressorLabel('Getting original query parameters...')        # check attributes        in_bands = list(ds.data_vars)        collections = list(ds.orig_collections)        bbox = list(ds.orig_bbox)        in_res = ds.res # use get xr res method        crs = ds.crs        in_slc_off = ds.orig_slc_off        resampling = ds.orig_resample        nodatavals = ds.nodatavals        # need to do        in_epsg = int(crs.split(':')[1])        in_platform = 'Landsat'        dtype = 'int16'        fill_value = -999        in_snap = True        rescale = True        cell_align = 'Top-left'        chunk_size = -1                # get datetimes        arcpy.SetProgressorLabel('Assessing dates...')        # get now, earliest, latest datetimes in dataset        dt_now = np.datetime64('now')        dt_first = ds['time'].isel(time=0).values        dt_last = ds['time'].isel(time=-1).values        # conver to stac format        in_from_date = arc.datetime_to_string(pd.Timestamp(dt_last))        in_to_date = arc.datetime_to_string(pd.Timestamp(dt_now))        # check if xr dt less than now (will be for now, but not if override)        if dt_last < dt_now:                        # fetch cog            arcpy.SetProgressorLabel('Performing STAC query...')            feats = cog.fetch_stac_data(stac_endpoint=STAC_ENDPOINT,                                         collections=collections,                                         start_dt=in_from_date,                                         end_dt=in_to_date,                                         bbox=bbox,                                        slc_off=in_slc_off,                                        limit=RESULT_LIMIT)                                                    # count number of items            arcpy.AddMessage('Found {} {} scenes.'.format(len(feats), in_platform))                                # prepare band (i.e. stac assets) names            assets = in_bands            #assets = arc.prepare_band_names(in_bands=in_bands,                                             #in_platform=in_platform)                            # convert raw stac into dict with coord reproject, etc.            arcpy.SetProgressorLabel('Converting STAC data into useable format...')            meta, asset_table = cog.prepare_data(feats,                                                  assets=assets,                                                 bounds_latlon=bbox,                                                  bounds=None,                                                  epsg=in_epsg,                                                  resolution=in_res,                                                  snap_bounds=in_snap,                                                 force_dea_http=True)                                                                                    else:            arcpy.AddMessage('No new scenes available. No sync required.')            returnclass COG_Explore(object):    def __init__(self):        self.label = "COG Explore"        self.description = "Explore an existing multidimensional " \                            "raster layer downloaded using COG " \                            "Fetcher."        self.canRunInBackground = False    def getParameterInfo(self):            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                        # set options        par_nc_file.filter.list = ['nc']        # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName="Set vegetation index",                        name="in_veg_idx",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                                # set options        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'        ]        par_veg_idx.value = 'MAVI'                               # input interpolate        par_interpolate = arcpy.Parameter(                            displayName="Interpolate missing pixels",                            name="in_interpolate",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                # set slc-off value        par_interpolate.value = True        # set oa class values        par_fmask_flags = arcpy.Parameter(displayName="Include flags",                                          name="in_fmask_flags",                                          datatype="GPString",                                          parameterType="Required",                                          direction="Input",                                          category='Quality Options',                                          multiValue=True                                          )        # set options        fmask_flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'        ]                # set default bands        par_fmask_flags.filter.type = "ValueList"                par_fmask_flags.filter.list = fmask_flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                            displayName="Maximum cloud cover",                            name="in_max_cloud",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            category='Quality Options',                            multiValue=False                            )                                    # set default platform        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0        # combine parameters        parameters = [            par_nc_file,            par_veg_idx,            par_interpolate,            par_fmask_flags,            par_max_cloud        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):                        # imports        import os, sys        import numpy as np        import xarray as xr        import arcpy        from datetime import datetime        from tempfile import NamedTemporaryFile                # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, tools, satfetcher                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import cog                # notify         arcpy.AddMessage('Opening COG Explore.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText      # raw netcdf        in_veg_idx = parameters[1].value       # vege index name        in_interpolate = parameters[2].value   # interpolate missing pixels        in_fmask_flags = parameters[3].valueAsText   # fmask flag values        in_max_cloud = parameters[4].value     # max cloud percentage        # set up progess bar        arcpy.SetProgressor(type='default', message='Loading and checking netcdf...')                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                # tcheck if vars/bands exist        if len(ds.data_vars) == 0:            raise ValueError('No satellite bands detected in NetCDF.')                        # obtain band attributes for any band        band_attrs = ds[list(ds.data_vars)[0]].attrs                # get platform of current netcdf, expects dea aws labels        collections = ds.attrs.get('orig_collections')                # todo: put in func. grab collections whether string or tuple/list        if isinstance(collections, (list, tuple)) and len(collections) > 0:            in_platform = collections[0]        elif isinstance(collections, str):            in_platform = collections        else:            raise ValueError('Input NetCDF missing DEA AWS metadata.')                    # parse dea aws platform code from collections attirbute        if in_platform[:5] == 'ga_ls':            in_platform = 'Landsat'        elif in_platform[:2] == 's2':            in_platform = 'Sentinel'        else:            raise ValueError('Platform in NetCDF is not supported.')        # set name of mask band depending on platform        if in_platform == 'Landsat':            mask_band = 'oa_fmask'        elif in_platform == 'Sentinel':            mask_band = 'fmask'        else:            raise ValueError('No DEA AWS compatible mask band detected.')                # todo : func to do all this (convert value to num etc)        flags = [e for e in in_fmask_flags.split(';')]                # unpack flags as indexes. todo: move to arc function        in_fmask_flags = []        for flag in flags:            if flag == 'NoData':                in_fmask_flags.append(0)            elif flag == 'Valid':                in_fmask_flags.append(1)            elif flag == 'Cloud':                in_fmask_flags.append(2)            elif flag == 'Shadow':                in_fmask_flags.append(3)            elif flag == 'Snow':                in_fmask_flags.append(4)            elif flag == 'Water':                in_fmask_flags.append(5)                                        # remove clouded pixels and empty dates        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                  # generate mavi and drop bands         arcpy.SetProgressorLabel('Conforming band names...')        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())                                                                                                                                                 # generate mavi and drop bands         arcpy.SetProgressorLabel('Calculating {}...'.format(in_veg_idx))        ds = tools.calculate_indices(ds=ds,                                      index=in_veg_idx.lower(),                                      custom_name=in_veg_idx.lower(),                                      rescale=False,                                      drop=True)                                          # add band attrs back on        ds[in_veg_idx.lower()].attrs = band_attrs                   # interpolate         if in_interpolate:            arcpy.SetProgressorLabel('Interpolating missing pixels...')            ds = ds.interpolate_na(dim='time', method='nearest')                # todo make this efficient        # create temp netcdf with clean data         with NamedTemporaryFile() as tmp:            fn = tmp.name + '.nc'            ds.to_netcdf(fn)                    # temp disable auto add of outputs to map        arcpy.env.addOutputsToMap = False                # prepare output filename and folder        in_folder = os.path.dirname(in_nc)        dt = datetime.now().strftime("%d%m%Y%H%M%S")        out_crf = os.path.join(in_folder, 'mdr' + '_' + dt + '.crf')                # export new multidim raster raster for visualise        mdr = arcpy.CopyRaster_management(in_raster=fn,                                           out_rasterdataset=out_crf)                                                  # todo make this safe        aprx = arcpy.mp.ArcGISProject('CURRENT')        m = aprx.activeMap        m.addDataFromPath(mdr)                    # re-enable auto add to map and apply cmap        arcpy.env.addOutputsToMap = True        lyr = arc.apply_cmap(aprx=aprx,                              lyr_name='mdr' + '_' + dt + '.crf',                              cmap_name='Precipitation',                              cutoff_pct=0.5)                                     # close and del dataset        ds.close()        del ds                returnclass GDVSpectra_Likelihood(object):    def __init__(self):            # set tool name        self.label = "GDVSpectra Likelihood"                # set tool description        self.description = "GDVSpectra Likelihood derives potential groundwater " \                           "dependent vegetation areas from three or more years of " \                           "Landsat or Sentinel NetCDF data. This functions results in " \                           "a single raster layer of GDV likelihood (0 to 1)."                                   # set false for pro        self.canRunInBackground = False    def getParameterInfo(self):                # input netcdf file (temp)        par_nc_path = arcpy.Parameter(                        displayName="Input NetCDF file",                        name="in_nc_path",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )        # set file type to be netcdf only        par_nc_path.filter.list = ['nc']                # output folder location        par_out_nc_path = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_path",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_path.filter.list = ['nc']                # in_wet_months        par_wet_months = arcpy.Parameter(                            displayName="Wet month(s)",                            name="in_wet_months",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default wet months        par_wet_months.values = '1, 2, 3'                # in_dry_months        par_dry_months = arcpy.Parameter(                            displayName="Dry month(s)",                            name="in_dry_months",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default dry months        par_dry_months.values = '9, 10, 11'                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName="Set vegetation index",                        name="in_veg_idx",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )         # set options        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # in_mst_idx        par_mst_idx = arcpy.Parameter(                        displayName="Set moisture index",                        name="in_mst_idx",                        datatype="GPString",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )                                    # set default mst idx        par_mst_idx.filter.type = 'ValueList'        par_mst_idx.filter.list = [            'NDMI',            'GVMI'            ]        par_mst_idx.value = 'NDMI'                    # input interpolate        par_interpolate = arcpy.Parameter(                            displayName="Interpolate missing pixels",                            name="in_interpolate",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                # set interpolate value        par_interpolate.value = True                # set oa class values        par_fmask_flags = arcpy.Parameter(displayName="Include flags",                                          name="in_fmask_flags",                                          datatype="GPString",                                          parameterType="Required",                                          direction="Input",                                          category='Quality Options',                                          multiValue=True                                          )        # set options        fmask_flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]                # set default bands        par_fmask_flags.filter.type = "ValueList"                par_fmask_flags.filter.list = fmask_flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                            displayName="Maximum cloud cover",                            name="in_max_cloud",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            category='Quality Options',                            multiValue=False                            )                                    # set default platform        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # set pvalue for zscore        par_zscore_pvalue = arcpy.Parameter(                                displayName="Z-score p-value",                                name="in_zscore_pvalue",                                datatype="GPDouble",                                parameterType="Optional",                                direction="Input",                                category='Outlier Correction',                                multiValue=False)                                    # set default mst idx        par_zscore_pvalue.value = None                # set q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                                displayName="Upper percentile",                                name="in_stand_qupper",                                datatype="GPDouble",                                parameterType="Optional",                                direction="Input",                                category='Standardisation',                                multiValue=False)                                    # set default mst idx        par_ivt_qupper.value = 0.99                # set q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                                displayName="Lower percentile",                                name="in_stand_qlower",                                datatype="GPDouble",                                parameterType="Optional",                                direction="Input",                                category='Standardisation',                                multiValue=False)                                    # set default mst idx        par_ivt_qlower.value = 0.05                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                    displayName="Add result to map",                                    name="in_add_result_to_map",                                    datatype="GPBoolean",                                    parameterType="Required",                                    direction="Input",                                    category='Outputs',                                    multiValue=False                                    )                # set add result to map value        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_nc_path,            par_out_nc_path,            par_wet_months,             par_dry_months,             par_veg_idx,             par_mst_idx,             par_interpolate,            par_fmask_flags,            par_max_cloud,            par_zscore_pvalue,            par_ivt_qupper,            par_ivt_qlower,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """                # imports        import os, sys        import numpy as np        import pandas as pd        import xarray as xr                # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, satfetcher, tools                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import gdvspectra, cog                # notify user        arcpy.AddMessage('Beginning GDVSpectra Likelihood.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText            # raw input satellite netcdf        out_nc = parameters[1].valueAsText           # output gdv likelihood netcdf        in_wet_months = parameters[2].value          # wet months         in_dry_months = parameters[3].value          # dry months         in_veg_idx = parameters[4].value             # vege index name        in_mst_idx = parameters[5].value             # moisture index name        in_interpolate = parameters[6].value         # interpolate missing pixels        in_fmask_flags = parameters[7].valueAsText   # fmask flag values        in_max_cloud = parameters[8].value           # max cloud percentage        in_zscore_pvalue = parameters[9].value       # zscore pvalue        in_add_result_to_map = parameters[10].value  # add result to map        # still need        #invar target q up        #invar target q low         #mask for stability                # set up progess bar        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0,                            max_range=18)                # prepare wet, dry season lists todo put into a func        wet_month = [int(e) for e in in_wet_months.split(',')]        dry_month = [int(e) for e in in_dry_months.split(',')]                # todo : func to do all this (convert value to num etc)        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = [1, 4, 5]        # set up progess bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(0)                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                                # remove clouded pixels and empty dates        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(1)        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band='oa_fmask',                                     nodata_value=np.nan,                                     drop_fmask=True)             # make copy for ds for later cva work        #ds_backup = ds.copy(deep=True)                                            # todo temp - nicer solution?        band_attrs = ds['nbart_blue'].attrs                # todo capture platform in cog fetcher and use here        in_platform = 'landsat'                # generate mavi and drop bands         arcpy.SetProgressorLabel('Conforming band names...')        arcpy.SetProgressorPosition(2)        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())              # reduce xr dataset into only wet, dry months        arcpy.SetProgressorLabel('Reducing dataset down to wet and dry months...')        arcpy.SetProgressorPosition(3)        ds = gdvspectra.subset_months(ds=ds,                                       month=wet_month + dry_month,                                      inplace=True)        # calculate vegetation and moisture indices        arcpy.SetProgressorLabel('Generating vege ({}) and moist ({}) indices...'.format(in_veg_idx, in_mst_idx))        arcpy.SetProgressorPosition(4)        ds = tools.calculate_indices(ds=ds,                                      index=[in_veg_idx.lower(), in_mst_idx.lower()],                                      custom_name=['veg_idx', 'mst_idx'],                                      rescale=True,                                      drop=True)                                       # add band attrs back on        ds['veg_idx'].attrs = band_attrs           ds['mst_idx'].attrs = band_attrs          # interpolate         if in_interpolate:            arcpy.SetProgressorLabel('Interpolating missing pixels...')            arcpy.SetProgressorPosition(5)            ds = ds.interpolate_na(dim='time', method='nearest')        # perform resampling        arcpy.SetProgressorLabel('Resampling dataset to annual wet/dry medians....')        arcpy.SetProgressorPosition(6)        ds = gdvspectra.resample_to_wet_dry_medians(ds=ds,                                                     wet_month=wet_month,                                                     dry_month=dry_month,                                                    inplace=True)                                                            # persist memory        arcpy.SetProgressorLabel('Persisting data into memory...')        arcpy.SetProgressorPosition(7)        ds = ds.persist()        # perform outlier removal        if in_zscore_pvalue is not None:            arcpy.SetProgressorLabel('Removing outliers via Z-Score...')            arcpy.SetProgressorPosition(8)            ds = gdvspectra.nullify_wet_dry_outliers(ds=ds,                                                      wet_month=wet_month,                                                      dry_month=dry_month,                                                      p_value=in_zscore_pvalue,  # todo: link this to param                                                     inplace=True)                                                             # remove any years missing wet, dry season         arcpy.SetProgressorLabel('Removing any years missing wet, dry seasons...')        arcpy.SetProgressorPosition(9)        ds = gdvspectra.drop_incomplete_wet_dry_years(ds=ds)        # fill any empty first, last years using back/forward fill        arcpy.SetProgressorLabel('Filling any empty first and last years...')        arcpy.SetProgressorPosition(10)        ds = ds.compute() # todo - bug in xarray with bfill, ffill and dask        ds = gdvspectra.fill_empty_wet_dry_edges(ds=ds,                                                 wet_month=wet_month,                                                  dry_month=dry_month,                                                 inplace=True)                                                         # interpolate missing values         arcpy.SetProgressorLabel('Interpolating missing values for wet, dry seasons...')        arcpy.SetProgressorPosition(11)        ds = gdvspectra.interp_empty_wet_dry(ds=ds,                                             wet_month=wet_month,                                             dry_month=dry_month,                                             method='full',          #todo link to param                                             inplace=True)                                                     # standardise data to invariant targets derived from dry times        arcpy.SetProgressorLabel('Standardising data to dry season invariant targets...')        arcpy.SetProgressorPosition(12)        ds = gdvspectra.standardise_to_dry_targets(ds=ds,                                                    dry_month=dry_month,                                                    q_upper=0.99,     # todo link to params                                                   q_lower=0.05,     # todo link to params                                                   inplace=True)                                                           # calculate seasonal similarity        arcpy.SetProgressorLabel('Calculating seasonal similarity...')        arcpy.SetProgressorPosition(13)        ds_similarity = gdvspectra.calc_seasonal_similarity(ds=ds,                                                            wet_month=wet_month,                                                            dry_month=dry_month,                                                            q_mask=0.9,           # todo link to params                                                            inplace=True)                                                                    # calculate gdv likelihood        arcpy.SetProgressorLabel('Calculating groundwater dependent vegetation likelihood...')        arcpy.SetProgressorPosition(14)        ds = gdvspectra.calc_likelihood(ds=ds,                                         ds_similarity=ds_similarity,                                        wet_month=wet_month,                                         dry_month=dry_month)        # temp - add attrs         ds['like'].attrs = band_attrs          # todo - temp - get median all time         #arcpy.SetProgressorLabel('Reducing result via median...')        #arcpy.SetProgressorPosition(15)        #ds = ds.median('time', keep_attrs=True)                # export netcdf to output location        arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(16)        tools.export_xr_as_nc(ds=ds, filename=out_nc)                # add to map if requested        if in_add_result_to_map:            arcpy.SetProgressorLabel('Adding result to map...')            arcpy.SetProgressorPosition(17)            # open netcdf as multidimensional raster             arcpy.env.addOutputsToMap = False            mdr = arcpy.CopyRaster_management(in_raster=out_nc,                                               out_rasterdataset='mdr_like.crf') # todo make dynamic                                                          # todo make this safe            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(mdr)               # apply cmap            arcpy.env.addOutputsToMap = True            lyr = arc.apply_cmap(aprx=aprx,                                  lyr_name='mdr_like.crf',                                  cmap_name='Bathymetric Scale',                                  cutoff_pct=0.01)                  # finalise process        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(18)        del ds, ds_similarity # ds_backup        # notify user        arcpy.AddMessage('Generated likelihood successfully.')                returnclass GDVSpectra_Threshold(object):    def __init__(self):        self.label = "GDVSpectra Threshold"        self.description = "Threshold an existing GDV Likelihood " \                            "cube using a shapefile of points or " \                            "a standard deviation value. Requires a " \                            "GDVSpectra Likelihood netcdf file as input."        self.canRunInBackground = False    def getParameterInfo(self):        """Text"""            # input netcdf file        par_nc_file = arcpy.Parameter(                        displayName="Input Likelihood NetCDF file",                        name="in_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                # set options        par_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_file",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_file.filter.list = ['nc']                # set use all dates checkbox        par_use_all_dates = arcpy.Parameter(                            displayName="Combine all dates in input",                            name="in_use_all_dates",                            datatype="GPBoolean",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set interpolate value        par_use_all_dates.value = True                # set specific year        par_specific_year = arcpy.Parameter(                            displayName="Select specific threshold year",                            name="in_specific_year",                            datatype="GPLong",                            parameterType="Optional",                            direction="Input",                            multiValue=False                            )                                    # set default nan handler        par_specific_year.filter.type = 'ValueList'        par_specific_year.filter.list = []        par_specific_year.value = None                # input occurrence points shapefile        par_occurrence_feat = arcpy.Parameter(                                displayName="Input occurrence point feature",                                name="in_occurrence_feat",                                datatype="GPFeatureLayer",                                parameterType="Optional",                                direction="Input"                                )                                        # set occurrence points to points only        par_occurrence_feat.filter.list = ['Point']                # set specific presence/absence column        par_pa_column = arcpy.Parameter(                            displayName="Set column of presence and absence values",                            name="in_pa_column",                            datatype="GPString",                            parameterType="Optional",                            direction="Input",                            multiValue=False                            )                                    # set default nan handler        par_pa_column.filter.type = 'ValueList'        par_pa_column.filter.list = []        #par_pa_column.value = None                # number of standard devs        par_std_dev = arcpy.Parameter(                            displayName="Standard deviation value",                            name="in_std_dev",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            multiValue=False                            )                                    # set default platform        par_std_dev.filter.type = 'Range'        par_std_dev.filter.list = [0.0, 10.0]        par_std_dev.value = 2.0                # set nan handler        par_if_nodata = arcpy.Parameter(                            displayName="Ignore pixel where NoData",                            name="in_if_nodata",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default nan handler        par_if_nodata.filter.type = 'ValueList'        par_if_nodata.filter.list = [            'Any',            'All'            ]        par_if_nodata.value = 'Any'                # set use all dates checkbox        par_remove_stray = arcpy.Parameter(                           displayName="Remove stray pixels",                           name="in_remove_stray",                           datatype="GPBoolean",                           parameterType="Required",                           direction="Input",                           multiValue=False                           )                                    # set interpolate value        par_remove_stray.value = True                # set use all dates checkbox        par_convert_binary = arcpy.Parameter(                             displayName="Convert output to binary",                             name="in_convert_binary",                             datatype="GPBoolean",                             parameterType="Required",                             direction="Input",                             multiValue=False                             )                                    # set interpolate value        par_convert_binary.value = True                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                    displayName="Add result to map",                                    name="in_add_result_to_map",                                    datatype="GPBoolean",                                    parameterType="Required",                                    direction="Input",                                    category='Outputs',                                    multiValue=False                                    )                # set add result to map value        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_nc_file,            par_out_nc_file,            par_use_all_dates,            par_specific_year,            par_occurrence_feat,            par_pa_column,            par_std_dev,            par_if_nodata,            par_remove_stray,            par_convert_binary,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""                return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                # imports        import xarray as xr                # update ui when use all dates unchecked        if not parameters[2].value:                    # disable specific year control            parameters[3].enabled = True                        # try and get nc if exists            years = []            try:                nc_path = parameters[0].valueastext                ds = xr.open_dataset(nc_path, chunks=-1)                years = list(ds.groupby('time.year').groups.keys())                years = [int(y) for y in years]            except:                arcpy.AddError('Could not read dates from input NetCDF. Is it broken?')                        # update values in control            parameters[3].filter.type = 'ValueList'            parameters[3].filter.list = years        else:            # disable specific year control            parameters[3].enabled = False                        # update values in control            parameters[3].filter.type = 'ValueList'            parameters[3].filter.list = []            parameters[3].value = None                    # update ui when use occurrence points        if parameters[4].value:                    # enable occurrence column, disable std dev control            parameters[5].enabled = True            parameters[6].enabled = False                        # update column name list            try:                shp_path = parameters[4].valueastext                col_names = [f.name for f in arcpy.ListFields(shp_path)]            except:                arcpy.AddError('Could not read fields from occurrence shapefile. Is it empty?')                        # update values in control            parameters[5].filter.type = 'ValueList'            parameters[5].filter.list = col_names                    else:            # disable occurrence column, enable std dev control            parameters[5].enabled = False            parameters[6].enabled = True                        # update values in control            parameters[5].filter.type = 'ValueList'            parameters[5].filter.list = []            parameters[5].values = None                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""                return    def execute(self, parameters, messages):        """Executes the GDV Spectra Threshold module."""                # temp! remove future warning on pandas        #import warnings        #warnings.simplefilter(action='ignore', category=FutureWarning)                        # imports        import os, sys        #import io        #import time        import numpy as np        import xarray as xr        from tempfile import NamedTemporaryFile        import arcpy        # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, tools, satfetcher                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import gdvspectra                # notify         arcpy.AddMessage('Beginning GDVSpectra Threshold.')                                                    # grab parameter values         in_nc = parameters[0].valueAsText                # likelihood netcdf        out_nc = parameters[1].valueAsText               # output netcdf        in_use_all_dates = parameters[2].value           # use all dates in nc         in_specific_year = parameters[3].value           # specific year         in_occurrence_feat = parameters[4]               # occurrence shp path         in_pa_column = parameters[5].value               # occurrence shp pres/abse col         in_std_dev = parameters[6].value                 # std dev threshold value         in_if_nodata = parameters[7].value               # handle no data as any or all         in_remove_stray = parameters[8].value            # apply salt n pepper -- requires sa        in_convert_binary = parameters[9].value          # convert thresh to binary 1, nan -- requires sa        in_add_result_to_map = parameters[10].value      # add result to map                # set up progess bar        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0,                            max_range=5)                                    # set up progess bar        arcpy.SetProgressorLabel('Reading provided occurrence shapefile...')        arcpy.SetProgressorPosition(0)        # read shapefile as pandas dataframe        df_records = None        if in_occurrence_feat.value is not None:            shp_desc = arcpy.Describe(in_occurrence_feat)            shp = os.path.join(shp_desc.path, shp_desc.name)                    # read shp as df            df_records = tools.read_shapefile(shp_path=shp)            # subset to just x, y, pres/abse column            df_records = tools.subset_records(df_records=df_records,                                               p_a_column=in_pa_column)                                                      # set up progess bar        arcpy.SetProgressorLabel('Reading Likelihood NetCDF.')        arcpy.SetProgressorPosition(1)                    # load netcdf file as xr        ds_like = satfetcher.load_local_nc(nc_path=in_nc,                                            use_dask=True,                                            conform_nodata_to=np.nan)                                                   # convert to dataset if data array        if isinstance(ds_like, xr.DataArray):            ds_like = ds_like.to_dataset('variable')                    # check if like variable exists         if 'like' not in list(ds_like.data_vars):            arcpy.AddError('Input NetCDF does not contain a likelihood variable.')                # get year or median of all time         # todo do to year if given        if 'time' in list(ds_like.dims):            if in_use_all_dates:                ds_like = ds_like.median('time', keep_attrs=True)            else:                ds_like = ds_like.where(ds_like['time.year'] == in_specific_year, drop=True)        else:            arcpy.AddMessage('NetCDF time dimension not detected. Using existing data.')                # set up progess bar        arcpy.SetProgressorLabel('Thresholding Likelihood...')        arcpy.SetProgressorPosition(2)                     # perform thresholding using shapefile points or std dev        if df_records is None:            ds_thresh = gdvspectra.threshold_likelihood(ds=ds_like,                                                        num_stdevs=in_std_dev,                                                         res_factor=3,                                                         if_nodata=in_if_nodata.lower())                                                                elif df_records is not None:            ds_thresh = gdvspectra.threshold_likelihood(ds=ds_like,                                                        df=df_records,                                                         res_factor=3,                                                         if_nodata=in_if_nodata.lower())                                                                # set up progess bar        arcpy.SetProgressorLabel('Converting Thresholded data to mask...')        arcpy.SetProgressorPosition(3)                        # convert to 1, 0 (int8 convert nan to 0) mask        ds_mask = ds_thresh.where(ds_thresh.isnull(), 1).astype('int8')                # export mask netcdf to output location        arcpy.SetProgressorLabel('Exporting NetCDF...')        arcpy.SetProgressorPosition(3)                    # also create mask netcdf        with NamedTemporaryFile() as tmp:            temp_mask_nc = tmp.name + '.nc'            tools.export_xr_as_nc(ds=ds_mask, filename=temp_mask_nc)                    # remove salt n pepper strays        if in_remove_stray:            arcpy.SetProgressorLabel('Removing stray pixels...')            arcpy.SetProgressorPosition(4)                        # apply filter if sa available            if arcpy.CheckExtension('Spatial') == 'Available':                arcpy.CheckOutExtension('Spatial')                                # open int mask netcdf as multidimensional raster                 arcpy.env.addOutputsToMap = False                mdr = arcpy.CopyRaster_management(in_raster=temp_mask_nc,                                                   out_rasterdataset='mdr_thresh.crf') # todo make dynamic                # apply salt n pepper                mdr_filter = arcpy.sa.MajorityFilter(mdr, 'FOUR', 'MAJORITY')                mdr_filter.save('in_memory/mdr_thresh_filter.crf')                            # copy raster into storage                mdr = arcpy.CopyRaster_management(in_raster='in_memory/mdr_thresh_filter.crf',                                                   out_rasterdataset='mdr_thresh.crf')                                          # get temp path for output nc and export                out_path = os.path.join(os.path.dirname(out_nc), 'temp.nc')                mask_nc = arcpy.md.RasterToNetCDF(in_raster=mdr,                                                   out_netCDF_file=out_path, # needs work                                                  variable='like',                                                  x_dimension='x',                                                  y_dimension='y')                                                                  # open temp nc                ds_mask = xr.open_dataset(out_path).compute()                                                                                         # delete any in memory files                 arcpy.Delete_management('in_memory')                arcpy.Delete_management(out_path)                                # check extension back in                arcpy.CheckInExtension("Spatial")                            else:                arcpy.AddWarning('Spatial Analyst is not available. Skipping filter.')                    # binary for ds_thresh        if in_convert_binary:            ds_thresh = ds_thresh.where(ds_thresh.isnull(), 1)                # add to map if requested        if in_add_result_to_map:            arcpy.SetProgressorLabel('Adding result to map...')            arcpy.SetProgressorPosition(5)                        # apply mask             ds_thresh = ds_thresh.where(ds_mask == 1, np.nan)                        # create thresh netcdf            with NamedTemporaryFile() as tmp:                temp_final_nc = tmp.name + '.nc'                tools.export_xr_as_nc(ds=ds_thresh, filename=temp_final_nc)                        # open netcdf as multidimensional raster             arcpy.env.addOutputsToMap = False            mdr = arcpy.CopyRaster_management(in_raster=temp_final_nc,                                               out_rasterdataset='mdr_thresh.crf') # todo make dynamic                                                          # todo make this safe            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(mdr)               # apply cmap            arcpy.env.addOutputsToMap = True            lyr = arc.apply_cmap(aprx=aprx,                                  lyr_name='mdr_thresh.crf',                                  cmap_name='Red-Blue (Continuous)',                                  cutoff_pct=0.01)                                               # set alpha if             lyr.transparency = 25                                                         # export netcdf to dest        ds_thresh.to_netcdf(out_nc)        # finalise process        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(4)                # del vars        del ds_like, ds_thresh, ds_mask, df_records                # notify user        arcpy.AddMessage('Generated likelihood threshold successfully.')                return# finish inputs, angle, magnitude, progressor barsclass GDVSpectra_Trend(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "GDVSpectra Trend"        self.description = "Perform a Mann-Kendall trend analysis on data cube."        self.canRunInBackground = False    # need pvalue and alpha value, mk direction    def getParameterInfo(self):        """Define parameter definitions"""        # input like netcdf file        par_like_nc_file = arcpy.Parameter(                        displayName="Input Likelihood NetCDF file",                        name="in_like_nc_file",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                # set options        par_like_nc_file.filter.list = ['nc']                # input netcdf file        par_mask_nc_file = arcpy.Parameter(                        displayName="Input Threshold Mask NetCDF file",                        name="in_mask_nc_file",                        datatype="DEFile",                        parameterType="Optional",                        direction="Input"                        )                                # set options        par_mask_nc_file.filter.list = ['nc']                # output netcdf location        par_out_nc_file = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_file",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_file.filter.list = ['nc']                # in_from_date        par_date_from = arcpy.Parameter(                            displayName="Date from",                            name="in_from_date",                            datatype="GPDate",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set in_from_date value        par_date_from.values = '2015/01/01'                # in_to_date        par_date_to = arcpy.Parameter(                        displayName="Date to",                        name="in_to_date",                        datatype="GPDate",                        parameterType="Required",                        direction="Input",                        multiValue=False                        )        # set in_from_date value        par_date_to.values = '2020/12/31'                # set analysis type        par_analysis_type = arcpy.Parameter(                            displayName="Trend method",                            name="in_analysis_type",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set analysis type        par_analysis_type.filter.type = 'ValueList'        par_analysis_type.filter.list = [            'Mann-Kendall',            'Theilsen Slope'            ]        par_analysis_type.value = 'Mann-Kendall'                # mk p-value        par_mk_pvalue = arcpy.Parameter(                            displayName="Mann-Kendall p-value",                            name="in_mk_pvalue",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            multiValue=False                            )                                    # set default platform        par_mk_pvalue.filter.type = 'Range'        par_mk_pvalue.filter.list = [0.001, 0.5]        par_mk_pvalue.value = None        # mk direction        par_mk_direction = arcpy.Parameter(                            displayName="Mann-Kendall trend direction",                            name="in_mk_direction",                            datatype="GPString",                            parameterType="Optional",                            direction="Input",                            multiValue=False                            )                                    # set default analysis type        par_mk_direction.filter.type = 'ValueList'        par_mk_direction.filter.list = [            'Both',            'Increasing',            'Decreasing'            ]        par_mk_direction.value = 'Both'                # ts alpha        par_ts_alpha = arcpy.Parameter(                        displayName="Theilsen alpha",                        name="in_ts_alpha",                        datatype="GPDouble",                        parameterType="Optional",                        direction="Input",                        multiValue=False                        )                                    # set default platform        par_ts_alpha.filter.type = 'Range'        par_ts_alpha.filter.list = [0.001, 0.5]        par_ts_alpha.value = 0.01                parameters = [            par_like_nc_file,            par_mask_nc_file,            par_out_nc_file,            par_date_from,            par_date_to,            par_analysis_type,            par_mk_pvalue,            par_mk_direction,            par_ts_alpha        ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    # todo finish start end dt updaters    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                import datetime        import xarray as xr                # if input nc selected, get start/end dt from dim and update inputs        #if parameters[0].valueAsText is not None:        if parameters[0].altered:            # get nc path, open it             nc_path = parameters[0].valueAsText            ds = xr.open_dataset(nc_path)                        try:                # convert array to dataset if detected                 if isinstance(ds, xr.DataArray):                    ds = ds.to_dataset(name='variable')                # check if got time dim, if so, pluck dts                if 'time' in list(ds.dims):                    start_dt = ds['time'].isel(time=0).dt.strftime('%Y-%m-%d').values                    end_dt = ds['time'].isel(time=-1).dt.strftime('%Y-%m-%d').values                                except:                # use placeholder, hope user works it out                start_dt, end_dt = str('01/01/2013'), str('31/12/2020')                                # update controls            parameters[3].value = str(start_dt)            parameters[4].value = str(end_dt)                # if start dt changed, check if >= to input nc        #if parameters[3].altered:                        # get nc path, open it             #nc_path = parameters[0].valueAsText            #ds = xr.open_dataset(nc_path)                        #try:                #1                # convert array to dataset if detected                 #if isinstance(ds, xr.DataArray):                    #ds = ds.to_dataset(name='variable')                # check if got time dim, if so, pluck dts                #if 'time' in list(ds.dims):                    #start_dt = ds['time'].isel(time=0).dt.strftime('%Y-%m-%d').values                    #ds_dt = datetime.datetime.strptime(start_dt)                    #control_dt = datetime.datetime.strptime(parameters[2].valueAsText)                                        #if control_dt < ds_dt:                        #parameters[2].value = str(start_dt)            #except:                # use placeholder, hope user works it out                #parameters[2].value = str('01/01/2013')                #2                                # update ui when use all dates unchecked        if parameters[5].valueAsText == 'Mann-Kendall':                    # show pvalue and direction            parameters[6].enabled = True            parameters[7].enabled = True                        # set theil sen alpha disabled            parameters[8].enabled = False                        # set values             parameters[6].value = None            parameters[7].value = 'Both'        else:            # disable pvalue and direction            parameters[6].enabled = False            parameters[7].enabled = False                        # set theil sen alpha disabled            parameters[8].enabled = True                        # set values             parameters[8].value = 0.01        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    # accept raw datacube and do mavi? like only at moment    def execute(self, parameters, messages):        """Executes the GDV Spectra Trend Analysis module."""        # temp! remove future warning on pandas        #import warnings        #warnings.simplefilter(action='ignore', category=FutureWarning)                        # imports        import os, sys        import numpy as np        import xarray as xr        from tempfile import NamedTemporaryFile        import scipy        import arcpy        # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, tools, satfetcher                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import gdvspectra                # notify         arcpy.AddMessage('Beginning GDVSpectra Trend Analysis.')                                                    # grab parameter values         in_like_nc = parameters[0].valueAsText         # likelihood netcdf        in_mask_nc = parameters[1].valueAsText         # thresh mask netcdf        out_nc = parameters[2].valueAsText             # output netcdf        in_from_date = parameters[3].value             # from date        in_to_date = parameters[4].value               # to date        in_trend_method = parameters[5].value          # trend method        in_mk_pvalue = parameters[6].value             # mk pvalue        in_mk_direction = parameters[7].value          # mk direction        in_ts_alpha = parameters[8].value              # ts alpha        # set up progess bar        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0,                            max_range=5)                                            # prepare start and end date times        in_from_date = arc.datetime_to_string(in_from_date)        in_to_date = arc.datetime_to_string(in_to_date)                # set up progess bar        arcpy.SetProgressorLabel('Reading Likelihood NetCDF.')        arcpy.SetProgressorPosition(1)                    # load netcdf file as xr        ds_like = satfetcher.load_local_nc(nc_path=in_like_nc,                                            use_dask=True,                                            conform_nodata_to=np.nan)                                                   # todo temp - nicer solution?        band_attrs = ds_like['like'].attrs                                                   # if mask provided, load that too        if in_mask_nc is not None:            ds_mask = satfetcher.load_local_nc(nc_path=in_mask_nc,                                                use_dask=True,                                                conform_nodata_to=np.nan)                                                                             ds_like = ds_like.where(~ds_mask['like'].isnull()) #todo this isnt safe                                                               # convert to dataset if data array        if isinstance(ds_like, xr.DataArray):            ds_like = ds_like.to_dataset('variable')                                # check if like variable exists         if 'like' not in list(ds_like.data_vars):            arcpy.AddError('Input NetCDF does not contain a likelihood variable.')                # perform trend method        if in_trend_method == 'Mann-Kendall':            if in_mk_direction == 'Both':                mk_dir = 'both'            elif in_mk_direction == 'Increasing':                mk_dir = 'inc'            else:                mk_dir = 'dec'                    # perform mk original            ds_trend = gdvspectra.perform_mk_original(ds=ds_like.compute(),                                                       pvalue=in_mk_pvalue,                                                       direction=mk_dir)                                                                  # append attrs back on             ds_trend['tau'].attrs = band_attrs                                                                elif in_trend_method == 'Theilsen Slope':                    # perform theil-sen             ds_trend = gdvspectra.perform_theilsen_slope(ds=ds_like.compute(),                                                          alpha=in_ts_alpha)            arcpy.AddMessage(ds_trend)            # append attrs back on             ds_trend['theilsen'].attrs = band_attrs                      # export netcdf to output location        #arcpy.SetProgressorLabel('Exporting NetCDF...')        #arcpy.SetProgressorPosition(2)        tools.export_xr_as_nc(ds=ds_trend, filename=out_nc)                # add to map if requested        in_add_result_to_map = True # add as param        if in_add_result_to_map:            #arcpy.SetProgressorLabel('Adding result to map...')            #arcpy.SetProgressorPosition(3)            # open netcdf as multidimensional raster             arcpy.env.addOutputsToMap = False            mdr = arcpy.CopyRaster_management(in_raster=out_nc,                                               out_rasterdataset='mdr_trend.crf') # todo make dynamic                                                          # todo make this safe            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(mdr)               # apply cmap            arcpy.env.addOutputsToMap = True            lyr = arc.apply_cmap(aprx=aprx,                                  lyr_name='mdr_trend.crf',                                  cmap_name='Red-Yellow-Blue (Continuous)',                                  cutoff_pct=0.0001)                  # finalise process        #arcpy.SetProgressorLabel('Finalising process...')        #arcpy.SetProgressorPosition(4)        del ds_like, ds_mask        # notify user        arcpy.AddMessage('Generated likelihood successfully.')                return# finish inputs, angle, magnitude, progressor barsclass GDVSpectra_CVA(object):    def __init__(self):        """Define the tool (tool name is the name of the class)."""        self.label = "GDVSpectra CVA"        self.description = "Perform a Change Vector Analysis (CVA) on data cube."        self.canRunInBackground = False    def getParameterInfo(self):                # input netcdf file (temp)        par_raw_nc_path = arcpy.Parameter(                        displayName="Input Satellite data NetCDF file",                        name="in_raw_nc_path",                        datatype="DEFile",                        parameterType="Required",                        direction="Input"                        )                                # set file type to be netcdf only        par_raw_nc_path.filter.list = ['nc']                # input netcdf mask (thresh) file        par_mask_nc_path = arcpy.Parameter(                        displayName="Input Threshold mask NetCDF file",                        name="in_mask_nc_path",                        datatype="DEFile",                        parameterType="Optional",                        direction="Input"                        )        # set file type to be netcdf only        par_mask_nc_path.filter.list = ['nc']                # output folder location        par_out_nc_path = arcpy.Parameter(                                displayName="Output NetCDF file",                                name="out_nc_path",                                datatype="DEFile",                                parameterType="Required",                                direction="Output"                                )                                        # set file type to be netcdf only        par_out_nc_path.filter.list = ['nc']                # in months        par_months = arcpy.Parameter(                            displayName="Set change month(s)",                            name="in_months",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default wet months        par_months.values = '9, 10, 11'                # base years        par_base_years = arcpy.Parameter(                            displayName="Set base image year(s)",                            name="in_base_years",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default base years        par_base_years.values = '(2013, 2013)'                # comparison years        par_comp_years = arcpy.Parameter(                            displayName="Set comparison image year(s)",                            name="in_comp_years",                            datatype="GPString",                            parameterType="Required",                            direction="Input",                            multiValue=False                            )                                    # set default comp years        par_comp_years.values = '(2015, 2020)'                # set q lower for standardisation        par_tmf = arcpy.Parameter(                                displayName="Threshold",                                name="in_tmf",                                datatype="GPDouble",                                parameterType="Required",                                direction="Input",                                multiValue=False)                                    # set default mst idx        par_tmf.value = 2                    # input interpolate        #par_interpolate = arcpy.Parameter(                            #displayName="Interpolate missing pixels",                            #name="in_interpolate",                            #datatype="GPBoolean",                            #parameterType="Required",                            #direction="Input",                            #multiValue=False                            #)                # set interpolate value        #par_interpolate.value = True                # set oa class values        par_fmask_flags = arcpy.Parameter(displayName="Include flags",                                          name="in_fmask_flags",                                          datatype="GPString",                                          parameterType="Required",                                          direction="Input",                                          category='Quality Options',                                          multiValue=True                                          )        # set options        fmask_flags = [            'NoData',             'Valid',             'Cloud',             'Shadow',             'Snow',             'Water'            ]                # set default bands        par_fmask_flags.filter.type = "ValueList"                par_fmask_flags.filter.list = fmask_flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # max cloud cover        par_max_cloud = arcpy.Parameter(                            displayName="Maximum cloud cover",                            name="in_max_cloud",                            datatype="GPDouble",                            parameterType="Optional",                            direction="Input",                            category='Quality Options',                            multiValue=False                            )                                    # set default platform        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # set pvalue for zscore        #par_zscore_pvalue = arcpy.Parameter(                                #displayName="Z-score p-value",                                #name="in_zscore_pvalue",                                #datatype="GPDouble",                                #parameterType="Optional",                                #direction="Input",                                #category='Outlier Correction',                                #multiValue=False)                                    # set default mst idx        #par_zscore_pvalue.value = None                # set q upper for standardisation        par_ivt_qupper = arcpy.Parameter(                                displayName="Upper percentile",                                name="in_stand_qupper",                                datatype="GPDouble",                                parameterType="Optional",                                direction="Input",                                category='Standardisation',                                multiValue=False)                                    # set default mst idx        par_ivt_qupper.value = 0.99                # set q lower for standardisation        par_ivt_qlower = arcpy.Parameter(                                displayName="Lower percentile",                                name="in_stand_qlower",                                datatype="GPDouble",                                parameterType="Optional",                                direction="Input",                                category='Standardisation',                                multiValue=False)                                    # set default mst idx        par_ivt_qlower.value = 0.05                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                    displayName="Add result to map",                                    name="in_add_result_to_map",                                    datatype="GPBoolean",                                    parameterType="Required",                                    direction="Input",                                    category='Outputs',                                    multiValue=False                                    )                # set add result to map value        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_raw_nc_path,            par_mask_nc_path,            par_out_nc_path,            par_months,            par_base_years,            par_comp_years,            par_tmf,            par_fmask_flags,            par_max_cloud,            par_ivt_qupper,            par_ivt_qlower,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """Modify the values and properties of parameters before internal        validation is performed.  This method is called whenever a parameter        has been changed."""                        return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the GDV Spectra Likelihood module.        """                # temp! remove future warning on pandas        #import warnings        #warnings.simplefilter(action='ignore', category=FutureWarning)                # imports        import os, sys        import numpy as np        import pandas as pd        import xarray as xr                # import tools        #sys.path.append('../../../shared') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')        import arc, satfetcher, tools                # import gdvspectra module        #sys.path.append('../../../modules') temp for demo        sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')        sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')        import gdvspectra, cog                # notify user        arcpy.AddMessage('Beginning GDVSpectra Likelihood.')                                                    # grab parameter values         in_raw_nc = parameters[0].valueAsText        # raw input satellite netcdf        in_mask_nc = parameters[1].valueAsText       # mask input satellite netcdf        out_nc = parameters[2].valueAsText           # output cva netcdf        in_months = parameters[3].value              # wet months         in_base_years = parameters[4].value          # in base years        in_comp_years = parameters[5].value          # in comp years        in_tmf = parameters[6].value                 # in tmf        in_fmask_flags = parameters[7].valueAsText   # fmask flag values        in_max_cloud = parameters[8].value           # max cloud percentage        in_add_result_to_map = parameters[9].value   # add result to map        # still need        #invar target q up        #invar target q low         #mask for stability                # set up progess bar        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                            min_range=0,                            max_range=18)                # prepare wet, dry season lists todo put into a func        in_months = [int(e) for e in in_months.split(',')]        # todo : func to do all this (convert value to num etc)        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = [1, 4, 5]        # set up progess bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(0)                # load netcdf file as xr        ds = satfetcher.load_local_nc(nc_path=in_raw_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                                              # do mask                                                # remove clouded pixels and empty dates        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(1)        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band='oa_fmask',                                     nodata_value=np.nan,                                     drop_fmask=True)             # make copy for ds for later cva work        #ds_backup = ds.copy(deep=True)                                            # todo temp - nicer solution?        band_attrs = ds['nbart_blue'].attrs                # todo capture platform in cog fetcher and use here        in_platform = 'landsat'                # generate mavi and drop bands         arcpy.SetProgressorLabel('Conforming band names...')        arcpy.SetProgressorPosition(2)        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())              # reduce xr dataset into only wet, dry months        arcpy.SetProgressorLabel('Reducing dataset down to wet and dry months...')        arcpy.SetProgressorPosition(3)        ds = gdvspectra.subset_months(ds=ds,                                       month=in_months,                                      inplace=True)        # calculate vegetation and moisture indices        arcpy.SetProgressorLabel('Generating vege ({}) and moist ({}) indices...'.format('tcg', 'tcb'))        arcpy.SetProgressorPosition(4)        ds = tools.calculate_indices(ds=ds,                                      index=['tcg', 'tcb'],                                      rescale=False,                                      drop=True)                                       # add band attrs back on        ds['tcg'].attrs = band_attrs           ds['tcb'].attrs = band_attrs                  # compute         ds = ds.compute()                # reduce all selected months into annual medians (year starts, YS)        ds = gdvspectra.resample_to_freq_medians(ds=ds,                                                 freq='YS',                                                 inplace=True)                                                                                                          # interpolate         #if in_interpolate:            #arcpy.SetProgressorLabel('Interpolating missing pixels...')            #arcpy.SetProgressorPosition(5)            #ds = ds.interpolate_na(dim='time', method='nearest')                # interpolate all missing pixels using full linear interpolation        ds = gdvspectra.interp_empty(ds=ds,                                     method='full',                                     inplace=True)                                             # standardise to targets        ds = gdvspectra.standardise_to_targets(ds,                                                q_upper=0.99,                                                q_lower=0.05)        # generate cva        ds_cva = gdvspectra.perform_cva(ds=ds,                                        base_times=(2013, 2013),                                        comp_times=(2016, 2020),                                        reduce_comp=False,                                        vege_var = 'tcg',                                        soil_var = 'tcb',                                        tmf=in_tmf)        # isolate angles between 90-180 degrees to focus on veg decline only        ds_veg_dec = gdvspectra.isolate_cva_change(ds_cva,                                                    angle_min=90,                                                    angle_max=180)                # isolate angles between 90-180 degrees to focus on veg decline only        ds_veg_inc = gdvspectra.isolate_cva_change(ds_cva,                                                    angle_min=270,                                                    angle_max=360)                                                           # isolate angles between 0-90 degrees to focus on moist decline only        ds_mst_dec = gdvspectra.isolate_cva_change(ds_cva,                                                    angle_min=0,                                                    angle_max=90)                # isolate angles between 90-180 degrees to focus on veg decline only        ds_veg_inc = gdvspectra.isolate_cva_change(ds_cva,                                                    angle_min=270,                                                    angle_max=360)            # isolate angles between 180-270 degrees to focus on moist increase only        ds_mst_inc = gdvspectra.isolate_cva_change(ds_cva,                                                    angle_min=180,                                                    angle_max=270)                                                                                                          # testing         ds_cva = ds_cva.rename({'angle': 'angle_all',                                 'magnitude': 'magnitude_all'})                ds_cva['veg_dec_magnitude'] = ds_veg_dec['magnitude']        ds_cva['veg_inc_magnitude'] = ds_veg_inc['magnitude']        ds_cva['mst_dec_magnitude'] = ds_mst_dec['magnitude']        ds_cva['mst_inc_magnitude'] = ds_mst_inc['magnitude']                # warning this doesnt work.        # create a mask where gdv highly likely        #ds_mask = xr.where(~ds_thresh.isnull(), True, False)        # select change areas where gdv exists for inc, dec        #ds_inc = ds_inc.where(ds_mask)        #ds_dec = ds_dec.where(ds_mask)                                                                # add band attrs back on        ds_cva['angle_all'].attrs = band_attrs           ds_cva['magnitude_all'].attrs = band_attrs         ds_cva['veg_dec_magnitude'].attrs = band_attrs        ds_cva['veg_inc_magnitude'].attrs = band_attrs              ds_cva['mst_dec_magnitude'].attrs = band_attrs              ds_cva['mst_inc_magnitude'].attrs = band_attrs                      # export netcdf to output location        #arcpy.SetProgressorLabel('Exporting NetCDF...')        #arcpy.SetProgressorPosition(16)        tools.export_xr_as_nc(ds=ds_cva, filename=out_nc)                # add to map if requested        if in_add_result_to_map:            arcpy.SetProgressorLabel('Adding result to map...')            arcpy.SetProgressorPosition(17)            # open netcdf as multidimensional raster             arcpy.env.addOutputsToMap = False            mdr = arcpy.CopyRaster_management(in_raster=out_nc,                                               out_rasterdataset='mdr_cva.crf') # todo make dynamic                                                          # todo make this safe            aprx = arcpy.mp.ArcGISProject('CURRENT')            m = aprx.activeMap            m.addDataFromPath(mdr)               # apply cmap            arcpy.env.addOutputsToMap = True            lyr = arc.apply_cmap(aprx=aprx,                                  lyr_name='mdr_cva.crf',                                  cmap_name='Spectrum by Wavelength-Full Bright',                                  cutoff_pct=0.01)                  # finalise process        #arcpy.SetProgressorLabel('Finalising process...')        #arcpy.SetProgressorPosition(18)        #del ds, ds_similarity # ds_backup        # notify user        arcpy.AddMessage('Generated CVA successfully.')                returnclass Phenolopy_Metrics(object):    def __init__(self):        self.label = 'Phenolopy Metrics'        self.description = 'Calculate various phenometrics from a data cube.'        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input netcdf file        par_raw_nc_path = arcpy.Parameter(                            displayName='Input Satellite data NetCDF file',                            name='in_raw_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Input')        par_raw_nc_path.filter.list = ['nc']                # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Phenometrics data Netcdf file',                            name='out_metrics_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                # input vegetation index         par_veg_idx = arcpy.Parameter(                        displayName='Vegetation index',                        name='in_veg_idx',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_veg_idx.filter.type = 'ValueList'        par_veg_idx.filter.list = [            'NDVI',            'EVI',             'SAVI',            'MSAVI',            'SLAVI',            'MAVI',            'kNDVI',            'TCG'            ]        par_veg_idx.value = 'MAVI'                # input time-series resampling        par_resampling_interval = arcpy.Parameter(                                    displayName='Resampling interval',                                    name='in_resampling_interval',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_resampling_interval.filter.type = 'ValueList'        par_resampling_interval.filter.list = [            'Weekly',            'Bi-monthly',             'Monthly'            ]        par_resampling_interval.value = 'Bi-monthly'                      # input metrics        par_metrics = arcpy.Parameter(                        displayName='Phenology metrics',                        name='in_metrics',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True)        metrics = [            'POS: Peak of season',            'MOS: Middle of season',            'VOS: Valley of season',            'BSE: Base of season',            'AOS: Amplitude of season',            'SOS: Start of season',            'EOS: End of season',            'LOS: Length of season',            'ROI: Rate of increase',            'ROD: Rate of decrease',            'LIOS: Long integral of season',            'SIOS: Short integral of season',            'LIOT: Long integral of total',            'SIOT: Short integral of total'            ]        par_metrics.filter.type = 'ValueList'                par_metrics.filter.list = metrics        par_metrics.values = metrics                       # input calculate nos        par_calc_nos = arcpy.Parameter(                         displayName='Calculate NOS (Num of seasons)',                         name='in_calc_nos',                         datatype='GPBoolean',                         parameterType='Required',                         direction='Input',                         multiValue=False)        par_calc_nos.value = False        # input method type        par_method_type = arcpy.Parameter(                            displayName='Method',                            name='in_method_type',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Other Method Options',                            multiValue=False)        par_method_type.filter.list = [            'First of slope',            'Median of slope',            'Absolute value',            'Seasonal amplitude',            'Relative amplitude'            ]        par_method_type.values = 'Median of slope'                # input peak detection type        par_peak_metric = arcpy.Parameter(                            displayName='Peak metric',                            name='in_peak_metric',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Other Method Options',                            multiValue=False)        par_peak_metric.filter.list = ['POS: Peak of season', 'MOS: Middle of season']        par_peak_metric.values = 'POS: Peak of season'            # input base detection type        par_base_metric = arcpy.Parameter(                            displayName='Base metric',                            name='in_base_metric',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Other Method Options',                            multiValue=False)        par_base_metric.filter.list = ['BSE: Base of season', 'VOS: Valley of season']        par_base_metric.values = 'VOS: Valley of season'                # input threshold side        par_threshold_side = arcpy.Parameter(                               displayName='Threshold side',                               name='in_threshold_side',                               datatype='GPString',                               parameterType='Required',                               direction='Input',                               category='Other Method Options',                               multiValue=False)        par_threshold_side.filter.list = ['One sided', 'Two sided']        par_threshold_side.values = 'Two sided'                # input seasonal amplitude factor. used only during  season amplitude method        par_seasonal_amp_factor = arcpy.Parameter(                                    displayName='Seasonal amplitude factor',                                    name='in_seasonal_amp_factor',                                    datatype='GPDouble',                                    parameterType='Optional',                                    direction='Input',                                    category='Other Method Options',                                    multiValue=False)        par_seasonal_amp_factor.filter.type = 'Range'        par_seasonal_amp_factor.filter.list = [0.0, 1.0]        par_seasonal_amp_factor.value = 0.5                # input absolute value. used only during absolute value method        par_absolute_value = arcpy.Parameter(                               displayName='Absolute value',                               name='in_absolute_value',                               datatype='GPDouble',                               parameterType='Optional',                               direction='Input',                               category='Other Method Options',                               multiValue=False)        par_absolute_value.value = 0.0        # input smooth method        par_smooth_method = arcpy.Parameter(                              displayName='Smoothing method',                              name='in_smooth_method',                              datatype='GPString',                              parameterType='Required',                              direction='Input',                              category='Smoothing',                              multiValue=False)        par_smooth_method.filter.list = ['Savitsky-Golay', 'Symmetrical Gaussian']        par_smooth_method.values = 'Savitsky-Golay'                 # input savitsky window length. only needed for savitsky        par_savitsky_window_length = arcpy.Parameter(                                       displayName='Window length',                                       name='in_savitsky_window_length',                                       datatype='GPLong',                                       parameterType='Required',                                       direction='Input',                                       category='Smoothing',                                       multiValue=False)        par_savitsky_window_length.filter.type = 'Range'        par_savitsky_window_length.filter.list = [3, 99]        par_savitsky_window_length.value = 3                 # input savitsky polyorder. only needed for savitsky        par_savitsky_polyorder = arcpy.Parameter(                                   displayName='Polyorder',                                   name='in_savitsky_polyorder',                                   datatype='GPLong',                                   parameterType='Required',                                   direction='Input',                                   category='Smoothing',                                   multiValue=False)        par_savitsky_polyorder.filter.type = 'Range'        par_savitsky_polyorder.filter.list = [1, 100]        par_savitsky_polyorder.value = 1                 # input symmetrical gaussian sigma. only needed for symm gauss        par_gaussian_sigma = arcpy.Parameter(                               displayName='Sigma',                               name='in_gaussian_sigma',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Smoothing',                               multiValue=False)        par_gaussian_sigma.filter.type = 'Range'        par_gaussian_sigma.filter.list = [1, 100]        par_gaussian_sigma.value = 1                        # input outlier removal method        par_outlier_method = arcpy.Parameter(                               displayName='Outlier removal method',                               name='in_outlier_method',                               datatype='GPString',                               parameterType='Required',                               direction='Input',                               category='Outlier Removal',                               multiValue=False)        par_outlier_method.filter.list = ['Local Median Threshold', 'Z-Score']        par_outlier_method.values = 'Local Median Threshold'                        # input user factor        par_user_factor = arcpy.Parameter(                            displayName='User factor',                            name='in_user_factor',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Outlier Removal',                            multiValue=False)        par_user_factor.filter.type = 'Range'        par_user_factor.filter.list = [1, 100]        par_user_factor.value = 2                                  # input zscore pvalue        par_zscore_pvalue = arcpy.Parameter(                              displayName='Zscore p-value',                              name='in_zscore_pvalue',                              datatype='GPDouble',                              parameterType='Required',                              direction='Input',                              category='Outlier Removal',                              multiValue=False)        par_zscore_pvalue.filter.type = 'Range'        par_zscore_pvalue.filter.list = [0.0001, 0.2]        par_zscore_pvalue.value = 0.05                      # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True        # combine parameters        parameters = [            par_raw_nc_path,            par_out_nc_path,            par_veg_idx,            par_resampling_interval,            par_metrics,            par_calc_nos,            par_method_type,            par_peak_metric,            par_base_metric,            par_threshold_side,            par_seasonal_amp_factor,            par_absolute_value,            par_smooth_method,            par_savitsky_window_length,            par_savitsky_polyorder,            par_gaussian_sigma,            par_outlier_method,            par_user_factor,            par_zscore_pvalue,            par_fmask_flags,             par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # enable seasonal amp factor if seasonal amp method selected        if parameters[6].value == 'Seasonal amplitude':            parameters[10].enabled = True  # enable seasonal amp factor        else:            parameters[10].enabled = False  # disable seasonal amp factor                    # enable absolute value if absolute value method selected        if parameters[6].value == 'Absolute value':            parameters[11].enabled = True  # enable absolute value        else:            parameters[11].enabled = False  # disable absolute value                    # enable window length and polyorder when savitsky method selected        if parameters[12].value == 'Savitsky-Golay':            parameters[13].enabled = True  # enable window lngth            parameters[14].enabled = True  # enable polyorder        else:            parameters[13].enabled = False  # disable window lngth            parameters[14].enabled = False  # disable polyorder                   # enable sigma when symmetrical gaussian method selected        if parameters[12].value == 'Symmetrical Gaussian':            parameters[15].enabled = True  # enable sigma        else:            parameters[15].enabled = False  # disable sigma                    # enable zscore pvalue when zscore selected        if parameters[16].value == 'Z-Score':            parameters[18].enabled = True  # enable zscore pvalue        else:            parameters[18].enabled = False  # disable zscore pvalue                return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the Phenolopy Metrics module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys       # arcgis comes with these        import datetime      # arcgis comes with this        import numpy as np   # arcgis comes with this        import pandas as pd  # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')            sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')            import arc, satfetcher, tools                          # module folder            sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')            sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')            import phenolopy, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise                                        # grab parameter values         in_nc = parameters[0].valueAsText                 # raw input satellite netcdf        out_nc = parameters[1].valueAsText                # output phenometrics netcdf        in_veg_idx = parameters[2].value                  # vege index name        in_resample_interval = parameters[3].value        # resample interval        in_metrics = parameters[4].valueAsText            # phenometrics        in_calc_nos = parameters[5].value                 # calculate nos        in_method_type = parameters[6].value              # phenolopy method type        in_peak_metric = parameters[7].value              # peak metric        in_base_metric = parameters[8].value              # base metric        in_threshold_side = parameters[9].value           # threshold side        in_seasonal_amp_factor = parameters[10].value     # seasonal amplitude factor        in_absolute_value = parameters[11].value          # absolute value        in_smooth_method = parameters[12].value           # smoothing method         in_sav_window_length = parameters[13].value       # savitsky window length         in_sav_polyorder = parameters[14].value           # savitsky polyorder         in_gaussian_sigma = parameters[15].value          # gaussian sigma        in_outlier_method = parameters[16].value          # outlier method        in_user_factor = parameters[17].value             # outlier cutoff user factor        in_zscore_pvalue = parameters[18].value           # zscore pvalue        in_fmask_flags = parameters[19].valueAsText       # fmask flag values        in_max_cloud = parameters[20].value               # max cloud percentage        in_add_result_to_map = parameters[21].value       # add result to map                # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning Phenolopy Metrics.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=17)        # convert resample interval to correct input (e.g. Bi-monthly to 1MS)        in_resample_interval = arc.convert_resample_interval_code(in_resample_interval)                # convert arcgis multi-value format to list of values        in_metrics = in_metrics.lower().replace("'", '').split(';')        in_metrics = [e.split(':')[0] for e in in_metrics]        # convert phenolopy method from arcgs to module friendly name        in_method_type = in_method_type.lower().replace(' ', '_')        # convert arcgis peak and base metrics format to module friendly names        in_peak_metric = in_peak_metric.lower().split(':')[0]        in_base_metric = in_base_metric.lower().split(':')[0]                # convert arcgis threshold side format to module friendly name        in_threshold_side = in_threshold_side.lower().replace(' ', '_')                # convert arcgis smoother name to module friendly name        in_smooth_method = arc.convert_smoother_code(in_smooth_method)                # check if savitsky parameters are correct, if selected        in_sav_window_length, in_sav_polyorder = arc.check_savitsky_inputs(in_sav_window_length,                                                                            in_sav_polyorder)                # convert arcgis outlier method to module friendly name        if in_outlier_method == 'Local Median Threshold':            in_outlier_method = 'median'        else:            in_outlier_method = 'zscore'                    # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking netcdf...')        arcpy.SetProgressorPosition(1)                # load raw netcdf        ds = satfetcher.load_local_nc(nc_path=in_nc,                                       use_dask=True,                                       conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            raise        else:            ds_attrs = ds.attrs            band_attrs = ds[list(ds.data_vars)[0]].attrs                    # check if expected band name exists (landsat/sentinel differences)        mask_band = arc.get_name_of_mask_band(list(ds.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and empty dates...')        arcpy.SetProgressorPosition(2)                # remove invalid pixels and empty scenes        ds = cog.remove_fmask_dates(ds=ds,                                     valid_class=in_fmask_flags,                                     max_invalid=in_max_cloud,                                     mask_band=mask_band,                                     nodata_value=np.nan,                                     drop_fmask=True)                                             # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_attrs)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating vegetation index...')        arcpy.SetProgressorPosition(3)                # conform dea aws band names based on platform        ds = satfetcher.conform_dea_ard_band_names(ds=ds,                                                    platform=in_platform.lower())         # calculate vegetation index         ds = tools.calculate_indices(ds=ds,                                      index=in_veg_idx.lower(),                                      custom_name='veg_idx',                                      rescale=False,                                      drop=True)                # append original attributes on to new band        ds['veg_idx'].attrs = band_attrs         # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Correcting edge dates...')        arcpy.SetProgressorPosition(4)                # ensure first/last date are start/end of year        ds = phenolopy.conform_edge_dates(ds=ds)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing a resample to equalise dates...')        arcpy.SetProgressorPosition(5)                # resample to weekly medians, prior to group-resample        ds = phenolopy.resample(ds=ds,                                 interval='1W',                                inplace=True)                        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Interpolating initial gaps...')        arcpy.SetProgressorPosition(6)                # interpolate missing values        ds = phenolopy.interpolate(ds=ds,                                    method='full',                                    inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Grouping times...')        arcpy.SetProgressorPosition(7)                # group and reduce dataset into median weeks (52 for one year)        ds = phenolopy.group(ds=ds,                              interval='week',                             inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing outliers...')        arcpy.SetProgressorPosition(8)                #  remove outliers from data using requeted method        ds = phenolopy.remove_outliers(ds=ds,                                        method=in_outlier_method,                                        user_factor=in_user_factor,                                        z_pval=in_zscore_pvalue)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Performing resample...')        arcpy.SetProgressorPosition(9)                # resample data using user interval        ds = phenolopy.resample(ds=ds,                                 interval=in_resample_interval,                                inplace=True)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing overshot times...')        arcpy.SetProgressorPosition(10)                # resampling can overshoot to previous/next year... remove those         ds = phenolopy.remove_overshoot_times(ds=ds, max_times=3)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Smoothing time-series...')        arcpy.SetProgressorPosition(11)               # use smoothing filter to smooth across time dimension        ds = phenolopy.smooth(ds=ds,                               method=in_smooth_method,                               window_length=in_sav_window_length,                               polyorder=in_sav_polyorder,                               sigma=in_gaussian_sigma)                                              # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Computing data into memory...')        arcpy.SetProgressorPosition(12)                # compute prior to phenometrics        ds = ds.compute()                        # # # # #        # calculate number of seasons, if requested        if in_calc_nos:                    # notify and increment progess bar            arcpy.SetProgressorLabel('Calculating number of seasons...')            arcpy.SetProgressorPosition(12)            # calculate number of seasons (num of major peaks) per-pixel            ds_nos = phenolopy.calc_num_seasons(ds=ds)                    # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating phenometrics...')        arcpy.SetProgressorPosition(13)                # calc phenometrics via phenolopy!        ds = phenolopy.calc_phenometrics(ds=ds,                                          metric=in_metrics,                                         peak_metric=in_peak_metric,                                          base_metric=in_base_metric,                                          method=in_method_type,                                          factor=in_seasonal_amp_factor,                                          thresh_sides=in_threshold_side,                                          abs_value=in_absolute_value)                                                 # add number of seasons to dataset if calculated        if in_calc_nos:            ds['nos_values'] = ds_nos['nos_values']                                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Appending attributes back on to dataset...')        arcpy.SetProgressorPosition(14)                # append attrbutes on to dataset and bands        ds.attrs = ds_attrs        for var in list(ds.data_vars):            ds[var].attrs = band_attrs                             # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Exporting NetCDF file...')        arcpy.SetProgressorPosition(15)                           # arcgis doesnt play nice with text-based dims...        try:            # replace with numeric            if 'variable' in list(ds.dims):                ds['variable'] = np.array([1])        except:            arcpy.AddWarning('Could not correct text-based dim.')        # export netcdf file        tools.export_xr_as_nc(ds=ds, filename=out_nc)                        # # # # #        # add multi-dim raster to current map        if in_add_result_to_map:                        # notify and increment progess bar            arcpy.SetProgressorLabel('Adding metrics to current ArcGIS map...')            arcpy.SetProgressorPosition(16)                                    # create output folder with dt            dt = datetime.datetime.now().strftime("%d%m%Y%H%M%S")            out_folder = os.path.join(os.path.dirname(out_nc), 'metrics' + '_' + dt)            os.makedirs(out_folder)                                   # enable auto-add to map for second            arcpy.env.addOutputsToMap = True                        try:                # try to get current map, fail if doesnt exist                aprx = arcpy.mp.ArcGISProject('CURRENT')                m = aprx.activeMap            except:                arcpy.AddMessage('No active map in ArcGIS.')                m = None            # loop each metric and export a seperate tif            if m is not None:                for metric in list(ds.data_vars):                                # create current output crf file                    out_crf = os.path.join(out_folder, metric + '.crf')                                        arcpy.AddMessage(out_crf)                                        try:                        # subset netcdf to current metric                        arcpy.md.SubsetMultidimensionalRaster(out_nc, out_crf, metric)                        m.addDataFromPath(out_crf)                                                                                                                                                    # determine optimal cmap based on data type                         if 'value' in metric:                            cmap, cutoff = 'Precipitation', 0.5                        else:                            cmap, cutoff = 'Temperature', 0.0                        # apply a colormap                        lyr = arc.apply_cmap(aprx=aprx,                                              lyr_name='{}.crf'.format(metric),                                             cmap_name=cmap,                                             cutoff_pct=cutoff)                                                                 except:                        arcpy.AddWarning('Could not visualise metric: {}.'.format(metric))        # # # # #        # clean up variables        arcpy.SetProgressorLabel('Finalising process...')        arcpy.SetProgressorPosition(17)                # close and del dataset        ds.close()        del ds        # notify user        arcpy.AddMessage('Generated phenometrics successfully.')        returnclass VegFrax_Fractional_Cover(object):    def __init__(self):        self.label = "VegFrax Fractional Cover"        self.description = "Extrapolate small areas of high resolution " \                           "classifed imagery across larger areas of lower " \                           "resolution imagery such as Landsat or Sentinel."        self.canRunInBackground = False    def getParameterInfo(self):        """        Set various ArcGIS Pro UI controls. Data validation        is enforced via ArcGIS Pro API.        """                # input low res satellite netcdf/tif file        par_low_res_path = arcpy.Parameter(                             displayName='Input lower resolution NetCDF file',                             name='in_low_res_path',                             datatype='DEFile',                             parameterType='Required',                             direction='Input')        par_low_res_path.filter.list = ['nc']        # input high res satellite tif file        par_high_res_path = arcpy.Parameter(                              displayName='Input classified high resolution GeoTIF file',                              name='in_high_res_path',                              datatype='DEFile',                              parameterType='Required',                              direction='Input')        par_high_res_path.filter.list = ['tif']                        # output netcdf file        par_out_nc_path = arcpy.Parameter(                            displayName='Output Fractional Cover Netcdf file',                            name='out_vegfrax_nc_path',                            datatype='DEFile',                            parameterType='Required',                            direction='Output')        par_out_nc_path.filter.list = ['nc']                      # input start year for low res netcdf/file        par_from_date = arcpy.Parameter(                                 displayName='Subset date of low resolution data from',                                 name='in_from_date',                                 datatype='GPDate',                                 parameterType='Required',                                 direction='Input',                                 multiValue=False)        par_from_date.values = '2015/01/01'                # input end year for low res netcdf/file        par_date_to = arcpy.Parameter(                        displayName='Subset date of low resolution data to',                        name='in_to_date',                        datatype='GPDate',                        parameterType='Required',                        direction='Input',                        multiValue=False)        par_date_to.values = '2020/12/31'                # input aggregator        par_aggregator = arcpy.Parameter(                                    displayName='Aggregator',                                    name='in_aggregator',                                    datatype='GPString',                                    parameterType='Required',                                    direction='Input',                                    multiValue=False)        par_aggregator.filter.type = 'ValueList'        par_aggregator.filter.list = [            'Mean',            'Median'            ]        par_aggregator.value = 'Median'                 # input classes list from high res netcdf/tif         par_classes = arcpy.Parameter(                        displayName='Set fractonal classes',                        name='in_classes',                        datatype='GPString',                        parameterType='Required',                        direction='Input',                        multiValue=True,                        enabled=False)        classes = ['No Classes']        par_classes.filter.type = 'ValueList'              par_classes.filter.list = classes        par_classes.values = ['Class: NoClass']                # input merge selected classes        par_merge_classes = arcpy.Parameter(                              displayName='Merge selected classes',                              name='in_merge_classes',                              datatype='GPBoolean',                              parameterType='Required',                              direction='Input',                              multiValue=False,                              enabled=False)        par_merge_classes.value = False                # input number of samples        par_num_samples = arcpy.Parameter(                            displayName='Number of random samples per class',                            name='in_num_samples',                            datatype='GPLong',                            parameterType='Required',                            direction='Input',                            category='Model Options',                            multiValue=False)        par_num_samples.filter.type = 'Range'        par_num_samples.filter.list = [10, 10000]        par_num_samples.value = 500                # input number of model estimators        par_num_estimators = arcpy.Parameter(                               displayName='Number of model estimators',                               name='in_num_estimators',                               datatype='GPLong',                               parameterType='Required',                               direction='Input',                               category='Model Options',                               multiValue=False)        par_num_estimators.filter.type = 'Range'        par_num_estimators.filter.list = [5, 1000]        par_num_estimators.value = 100                # input number of model validations        par_num_validations = arcpy.Parameter(                                displayName='Number of model validations',                                name='in_num_validations',                                datatype='GPLong',                                parameterType='Required',                                direction='Input',                                category='Model Options',                                multiValue=False)        par_num_validations.filter.type = 'Range'        par_num_validations.filter.list = [1, 100]        par_num_validations.value = 10                # input oa fmask         par_fmask_flags = arcpy.Parameter(                            displayName='Include flags',                            name='in_fmask_flags',                            datatype='GPString',                            parameterType='Required',                            direction='Input',                            category='Satellite Quality',                            multiValue=True)        flags = ['NoData', 'Valid', 'Cloud', 'Shadow', 'Snow', 'Water']        par_fmask_flags.filter.type = 'ValueList'              par_fmask_flags.filter.list = flags        par_fmask_flags.values = ['Valid', 'Snow', 'Water']                # input max cloud cover        par_max_cloud = arcpy.Parameter(                          displayName='Maximum cloud cover',                          name='in_max_cloud',                          datatype='GPDouble',                          parameterType='Optional',                          direction='Input',                          category='Satellite Quality',                          multiValue=False)        par_max_cloud.filter.type = 'Range'        par_max_cloud.filter.list = [0.0, 100.0]        par_max_cloud.value = 10.0                # input add result to map         par_add_result_to_map = arcpy.Parameter(                                  displayName='Add result to map',                                  name='in_add_result_to_map',                                  datatype='GPBoolean',                                  parameterType='Required',                                  direction='Input',                                  category='Outputs',                                  multiValue=False)        par_add_result_to_map.value = True                # combine parameters        parameters = [            par_low_res_path,            par_high_res_path,            par_out_nc_path,            par_from_date,            par_date_to,            par_aggregator,            par_classes,            par_merge_classes,            par_num_samples,            par_num_estimators,            par_num_validations,            par_fmask_flags,            par_max_cloud,            par_add_result_to_map            ]                return parameters    def isLicensed(self):        """Set whether tool is licensed to execute."""        return True    def updateParameters(self, parameters):        """        Enable and disable certain parameters when        controls are changed on ArcGIS Pro panel.        """                # risk imports (non-native to arcgis)        try:            import numpy as np   # this is in arcgis, but ok            import xarray as xr  # not in arcgis            import rasterio      # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                    # update ui datetimes input ds altered - only do on first input change        if parameters[0].altered and not parameters[0].hasBeenValidated:            try:                # get nc path, load nc                 nc_path = parameters[0].valueAsText                ds = xr.open_dataset(nc_path)                                # convert array to dataset if array detected                 if isinstance(ds, xr.DataArray):                    ds = ds.to_dataset(name='variable')                      # check if got time dim, if so, pluck dts                if 'time' in list(ds.dims):                    start_dt = ds['time'].isel(time=0).dt.strftime('%Y-%m-%d').values                    end_dt = ds['time'].isel(time=-1).dt.strftime('%Y-%m-%d').values                # update date controls                parameters[3].value = str(start_dt)                parameters[4].value = str(end_dt)            except:                arcpy.AddError('Could not open low resolution input NetCDF.')                raise                        # update ui classes selector list if high res altered - only do first change        if parameters[1].altered and not parameters[1].hasBeenValidated:            try:                # open provided high res tif                 tif_path = parameters[1].valueAsText                ds = xr.open_rasterio(tif_path)                                                   # check bands, if single, create classes list (strings)                if len(ds) != 1:                    arcpy.AddError('High resolution raster can not be multiband.')                    raise                                # check if nodata value embedded in xr                if not hasattr(ds, 'nodatavals'):                    arcpy.AddError('Dataset does not have nodata value attribute.')                    raise                elif ds.nodatavals == 'unknown':                    arcpy.AddError('Dataset nodata value is unknown.')                    raise                                    # get all unique classes in dataset and remove nodata                classes = np.unique(ds)                classes = classes[classes != ds.nodatavals]                                # check if we got something                if len(classes) <= 0:                    arcpy.AddError('No classes detected in dataset.')                    raise                                    # finally, load all non-nodata classes into ui                text_classes = []                for c in classes:                    text_classes.append('Class: {}'.format(c))                                        # do best to sort alphabetically                text_classes.sort()                # update classes ui control with new classes and enable                parameters[6].enabled = True                parameters[6].filter.list = text_classes                parameters[6].values = text_classes                            except:                arcpy.AddError('Could not open high resolution input GeoTIFF.')                raise                  # update ui merge selected classes when classes selector is enabled        if parameters[6].enabled:            parameters[7].enabled = True  # enable merge classes option        else:            parameters[7].enabled = False  # disable merge classes option                      return    def updateMessages(self, parameters):        """Modify the messages created by internal validation for each tool        parameter.  This method is called after internal validation."""        return    def execute(self, parameters, messages):        """        Executes the VegFrax Fractional Cover module.        """                # disable future warnings        import warnings        warnings.simplefilter(action='ignore', category=FutureWarning)                # safe imports        import os, sys       # arcgis comes with these        import datetime      # arcgis comes with this        import numpy as np   # arcgis comes with this        import pandas as pd  # arcgis comes with this        import datetime      # arcgis comes with this        # risk imports (non-native to arcgis)        try:            import xarray as xr  # not in arcgis        except:            arcpy.AddError('Python library Xarray is not installed.')            raise                # import tools        try:            # shared folder            sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\shared')            sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\shared')            import arc, satfetcher, tools                          # module folder            sys.path.append(r'C:\Users\262272G\Documents\GitHub\tenement-tools\modules')            sys.path.append(r'C:\Users\Lewis\Documents\GitHub\tenement-tools\modules')            import vegfrax, cog                      except:            arcpy.AddError('Could not find tenement tools python scripts (modules, shared).')            raise        # grab parameter values         in_low_res_nc = parameters[0].valueAsText         # raw input low res satellite netcdf        in_high_res_tif = parameters[1].valueAsText       # raw input high res satellite tif        out_nc = parameters[2].valueAsText                # output vegfrax netcdf        in_from_date = parameters[3].value                # start date of aggregate        in_to_date = parameters[4].value                  # end date of aggregate        in_aggregator = parameters[5].value               # aggregator        in_classes = parameters[6].valueAsText            # selected classes        in_merge_classes = parameters[7].value            # merge selected classes               in_num_samples = parameters[8].value              # number of samples        in_num_estimators = parameters[9].value           # number of model estimators        in_num_validators = parameters[10].value          # number of model validators        in_fmask_flags = parameters[11].valueAsText       # fmask flag values        in_max_cloud = parameters[12].value               # max cloud percentage        in_add_result_to_map = parameters[13].value       # add result to map        # # # # #        # notify user and set up progress bar        arcpy.AddMessage('Beginning VegFrax Fractional Cover.')        arcpy.SetProgressor(type='step',                             message='Preparing parameters...',                             min_range=0, max_range=17)        # convert datetime strings to numpy datetime64        in_from_date = arc.datetime_to_numpy(in_from_date)        in_to_date = arc.datetime_to_numpy(in_to_date)               # convert arcgis multi-value format to list of values and notify               in_classes = in_classes.replace('Class: ', '').replace("'", "").split(';')                # convert fmask flags as text to numeric code equivalents        in_fmask_flags = [e for e in in_fmask_flags.split(';')]        in_fmask_flags = arc.convert_fmask_codes(in_fmask_flags)        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Loading and checking low resolution netcdf...')        arcpy.SetProgressorPosition(1)                # load low res netcdf. set nodata to nan to mimic dea odc        ds_low = satfetcher.load_local_nc(nc_path=in_low_res_nc,                                           use_dask=True,                                           conform_nodata_to=np.nan)                                              # check netcdf if it has bands, get attributes for ds and a band        if len(ds_low.data_vars) == 0:            arcpy.AddError('No bands/variables detected in input NetCDF.')            raise        else:            ds_low_attrs = ds_low.attrs            ds_low_band_attrs = ds_low[list(ds_low.data_vars)[0]].attrs                    # check if expected band name exists (landsat/sentinel differences)        ds_low_mask_band = arc.get_name_of_mask_band(list(ds_low.data_vars))        # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Removing invalid pixels and dates from low res NetCDF...')        arcpy.SetProgressorPosition(2)                # remove invalid pixels and empty scenes        ds_low = cog.remove_fmask_dates(ds=ds_low,                                         valid_class=in_fmask_flags,                                         max_invalid=in_max_cloud,                                         mask_band=ds_low_mask_band,                                         nodata_value=np.nan,                                         drop_fmask=True)                                                     # conform and prepare low res dataset         ds_low = vegfrax.prepare_raw_xr(ds_low,                                         dtype='float32',                                         conform_nodata_to=-999)                # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('Calculating tasselled cap...')        arcpy.SetProgressorPosition(3)                # get platform name from attributes, error if no attributes        in_platform = arc.get_platform_from_dea_attrs(ds_low_attrs)                # conform dea aws band names based on platform        ds_low = satfetcher.conform_dea_ard_band_names(ds=ds_low,                                                        platform=in_platform.lower())         # calculate tasselled cap index         ds_low = tools.calculate_indices(ds=ds_low,                                          index=['tcg', 'tcb', 'tcw'],                                          custom_name=None,                                          rescale=False,                                          drop=True)                # append original attributes on to the new bands        ds_low['tcg'].attrs = ds_low_band_attrs         ds_low['tcb'].attrs = ds_low_band_attrs         ds_low['tcw'].attrs = ds_low_band_attrs         # # # # #        # notify and increment progess bar        arcpy.SetProgressorLabel('...')        arcpy.SetProgressorPosition(4)                                                                # restrict date range        #ds.where((ds['time'] >= in_from_date) & (ds['time'] <= in_to_date), drop=True)                        return