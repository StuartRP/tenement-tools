{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os, sys\n",
    "import io\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.array as da\n",
    "import rasterio\n",
    "import arcpy\n",
    "\n",
    "# import tools\n",
    "#sys.path.append('../../../shared') temp for demo\n",
    "sys.path.append(r'C:\\Users\\262272G\\Documents\\GitHub\\tenement-tools\\shared')\n",
    "sys.path.append(r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\shared')\n",
    "import arc, tools, satfetcher\n",
    "\n",
    "# import gdvspectra module\n",
    "#sys.path.append('../../../modules') temp for demo\n",
    "sys.path.append(r'C:\\Users\\262272G\\Documents\\GitHub\\tenement-tools\\modules')\n",
    "sys.path.append(r'C:\\Users\\Lewis\\Documents\\GitHub\\tenement-tools\\modules')\n",
    "import cog\n",
    "\n",
    "# globals \n",
    "AWS_KEY = ''\n",
    "AWS_SECRET = ''\n",
    "STAC_ENDPOINT = 'https://explorer.sandbox.dea.ga.gov.au/stac/search'\n",
    "RESULT_LIMIT = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning STAC search for items. This can take awhile.\n",
      "Searching collection: s2a_ard_granule\n",
      "Searching collection: s2b_ard_granule\n",
      "Sorting result by time (old to new).\n",
      "Found a total of 148 scenes.\n"
     ]
    }
   ],
   "source": [
    "# get minimum bounding geom from input \n",
    "bbox = [119.32132692558059, -22.774853394924776, 119.34873641649256, -22.745010351081362]\n",
    "\n",
    "# get collections based on platform \n",
    "collections = ['s2a_ard_granule', 's2b_ard_granule']\n",
    "\n",
    "# prepare start and end date times\n",
    "in_from_date = '2020-01-01'\n",
    "in_to_date = '2020-12-31'\n",
    "\n",
    "in_slc_off = False\n",
    "\n",
    "RESULT_LIMIT = 250\n",
    "\n",
    "# fetch stac data \n",
    "feats = cog.fetch_stac_data(stac_endpoint=STAC_ENDPOINT, \n",
    "                            collections=collections, \n",
    "                            start_dt=in_from_date, \n",
    "                            end_dt=in_to_date, \n",
    "                            bbox=bbox,\n",
    "                            slc_off=in_slc_off,\n",
    "                            limit=RESULT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_bands = 'Blue;Green;Red;NIR1;SWIR2;SWIR3;OA_Mask'\n",
    "in_platform = 'Sentinel'\n",
    "\n",
    "# prepare band (i.e. stac assets) names\n",
    "assets = arc.prepare_band_names(in_bands=in_bands, \n",
    "                                in_platform=in_platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting raw STAC data into numpy format.\n",
      "Converted raw STAC data successfully.\n"
     ]
    }
   ],
   "source": [
    "in_epsg = 3577\n",
    "in_res = 10\n",
    "in_snap = True\n",
    "\n",
    "# convert raw stac into dict with coord reproject, etc.\n",
    "arcpy.SetProgressorLabel('Converting STAC data into useable format...')\n",
    "meta, asset_table = cog.prepare_data(feats, \n",
    "                                     assets=assets,\n",
    "                                     bounds_latlon=bbox, \n",
    "                                     bounds=None, \n",
    "                                     epsg=in_epsg, \n",
    "                                     resolution=in_res, \n",
    "                                     snap_bounds=in_snap,\n",
    "                                     force_dea_http=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data into dask array.\n",
      "Converted data successfully.\n"
     ]
    }
   ],
   "source": [
    "in_resampling = 'Nearest'\n",
    "in_fill_value = '-999'\n",
    "in_chunk_size = -1\n",
    "in_dtype = 'int16'\n",
    "in_rescale = True\n",
    "\n",
    "# prepare resample and fill value types\n",
    "resampling = in_resampling.lower()\n",
    "fill_value = arc.prepare_fill_value_type(in_fill_value)\n",
    "\n",
    "# convert assets to dask array\n",
    "darray = cog.convert_to_dask(meta=meta, \n",
    "                             asset_table=asset_table, \n",
    "                             chunksize=in_chunk_size,\n",
    "                             resampling=resampling, \n",
    "                             dtype=in_dtype, \n",
    "                             fill_value=fill_value, \n",
    "                             rescale=in_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset coordinates and dimensions.\n",
      "Created coordinates and dimensions successfully.\n"
     ]
    }
   ],
   "source": [
    "in_cell_align = 'Top-left'\n",
    "\n",
    "# prepare alignment type\n",
    "cell_align = arc.prepare_cell_align_type(in_cell_align)\n",
    "\n",
    "# generate coordinates and dimensions from metadata\n",
    "coords, dims = cog.build_coords(feats=feats,\n",
    "                                assets=assets, \n",
    "                                meta=meta,\n",
    "                                pix_loc=cell_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build final xarray data array\n",
    "ds_name = 'stac-' + dask.base.tokenize(darray)\n",
    "ds = xr.DataArray(darray,\n",
    "                  coords=coords,\n",
    "                  dims=dims,\n",
    "                  name=ds_name\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing and appending attributes to dataset.\n",
      "Attributes appended to dataset successfully.\n"
     ]
    }
   ],
   "source": [
    "# comvert to cleaner xarray dataset\n",
    "ds = ds.to_dataset(dim='band')\n",
    "\n",
    "# append attributes onto dataset\n",
    "ds = cog.build_attributes(ds=ds,\n",
    "                          meta=meta, \n",
    "                          fill_value=fill_value, \n",
    "                          collections=collections, \n",
    "                          slc_off=in_slc_off, \n",
    "                          bbox=bbox,\n",
    "                          resampling=in_resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of dataset vars and iterate compute on each\n",
    "for counter, data_var in enumerate(list(ds.data_vars), start=1):\n",
    "\n",
    "    # start clock\n",
    "    start = time.time()\n",
    "\n",
    "    # update progress bar\n",
    "    print('Downloading band: {}...'.format(data_var))\n",
    "\n",
    "    # compute!\n",
    "    ds[data_var] = ds[data_var].compute()\n",
    "\n",
    "    # notify time \n",
    "    duration = round(time.time() - start, 2)\n",
    "    print('Band: {} took: {}s to download.'.format(data_var, duration)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try doing all in one go\n",
    "# start clock\n",
    "start = time.time()\n",
    "\n",
    "# update progress bar\n",
    "print('Downloading dataset...')\n",
    "\n",
    "# compute!\n",
    "ds = ds.compute()\n",
    "\n",
    "# notify time \n",
    "duration = round(time.time() - start, 2)\n",
    "print('Dataset took: {}s to download.'.format(duration)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "\n",
    "def compute_dataarray(var):\n",
    "    return ds[var].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset took: 260.77s to download.\n"
     ]
    }
   ],
   "source": [
    "# start clock\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "     result = executor.map(compute_dataarray, [var for var in ds.data_vars])\n",
    "        \n",
    "# show me time\n",
    "duration = round(time.time() - start, 2)\n",
    "print('Dataset took: {}s to download.'.format(duration)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir_1', 'nbart_swir_2', 'nbart_swir_3', 'fmask']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
